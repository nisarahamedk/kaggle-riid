{"cells":[{"metadata":{"id":"view-in-github"},"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/nisarahamedk/kaggle-riid/blob/master/notebooks/RIID_TF_Transformer_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"},{"metadata":{"id":"AIIMFhdG1L1o"},"cell_type":"markdown","source":"### RIID TF Transformer Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!rm -f /opt/conda/lib/python3.7/site-packages/llvmlite-0.31.0-py3.7.egg-info\n!pip install ../input/sparse-package-1/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl\n!pip install ../input/sparse-package-1/setuptools-51.1.1-py3-none-any.whl\n!pip install ../input/sparse-package-1/scipy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/sparse-package-1/llvmlite-0.35.0-cp37-cp37m-manylinux2010_x86_64.whl\n!pip install ../input/sparse-package-1/numba-0.52.0-cp37-cp37m-manylinux2014_x86_64.whl\n!pip install ../input/sparse-package-1/sparse-0.11.2-py2.py3-none-any.whl","execution_count":2,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/sparse-package-1/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.18.5\n    Uninstalling numpy-1.18.5:\n      Successfully uninstalled numpy-1.18.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnumba 0.48.0 requires llvmlite<0.32.0,>=0.31.0dev0, which is not installed.\ntensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.4 which is incompatible.\nbokeh 2.2.3 requires tornado>=5.1, but you have tornado 5.0.2 which is incompatible.\u001b[0m\nSuccessfully installed numpy-1.19.4\nProcessing /kaggle/input/sparse-package-1/setuptools-51.1.1-py3-none-any.whl\nInstalling collected packages: setuptools\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 46.1.3.post20200325\n    Uninstalling setuptools-46.1.3.post20200325:\n      Successfully uninstalled setuptools-46.1.3.post20200325\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnumba 0.48.0 requires llvmlite<0.32.0,>=0.31.0dev0, which is not installed.\nkubernetes 10.1.0 requires pyyaml~=3.12, but you have pyyaml 5.3.1 which is incompatible.\nearthengine-api 0.1.244 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\nSuccessfully installed setuptools-51.1.1\nProcessing /kaggle/input/sparse-package-1/scipy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy==1.6.0) (1.19.4)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.4.1\n    Uninstalling scipy-1.4.1:\n      Successfully uninstalled scipy-1.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nautogluon-core 0.0.15b20201207 requires scipy<1.5.0,>=1.3.3, but you have scipy 1.6.0 which is incompatible.\u001b[0m\nSuccessfully installed scipy-1.6.0\nProcessing /kaggle/input/sparse-package-1/llvmlite-0.35.0-cp37-cp37m-manylinux2010_x86_64.whl\nInstalling collected packages: llvmlite\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnumba 0.48.0 requires llvmlite<0.32.0,>=0.31.0dev0, but you have llvmlite 0.35.0 which is incompatible.\u001b[0m\nSuccessfully installed llvmlite-0.35.0\nProcessing /kaggle/input/sparse-package-1/numba-0.52.0-cp37-cp37m-manylinux2014_x86_64.whl\nRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from numba==0.52.0) (1.19.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba==0.52.0) (51.1.1)\nRequirement already satisfied: llvmlite<0.36,>=0.35.0 in /opt/conda/lib/python3.7/site-packages (from numba==0.52.0) (0.35.0)\nInstalling collected packages: numba\n  Attempting uninstall: numba\n    Found existing installation: numba 0.48.0\n    Uninstalling numba-0.48.0:\n      Successfully uninstalled numba-0.48.0\nSuccessfully installed numba-0.52.0\nProcessing /kaggle/input/sparse-package-1/sparse-0.11.2-py2.py3-none-any.whl\nRequirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.7/site-packages (from sparse==0.11.2) (0.52.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sparse==0.11.2) (1.19.4)\nRequirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.7/site-packages (from sparse==0.11.2) (1.6.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->sparse==0.11.2) (51.1.1)\nRequirement already satisfied: llvmlite<0.36,>=0.35.0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->sparse==0.11.2) (0.35.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sparse==0.11.2) (1.19.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sparse==0.11.2) (1.19.4)\nInstalling collected packages: sparse\nSuccessfully installed sparse-0.11.2\n","name":"stdout"}]},{"metadata":{"id":"KyaJuEug1SjH","trusted":true},"cell_type":"code","source":"import itertools\nimport pickle\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport pandas as pd\nimport numpy as np\n# import sparse\n\nnp.random.seed(42)\ntf.random.set_seed(42)","execution_count":1,"outputs":[]},{"metadata":{"id":"djh3Uu2t5gW0","trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE","execution_count":2,"outputs":[]},{"metadata":{"id":"FqfXTHol5tsO"},"cell_type":"markdown","source":"### Model"},{"metadata":{"id":"N7Etv2NO5zT2"},"cell_type":"markdown","source":"#### Positional Encoding"},{"metadata":{"id":"n6vaouEE53Ht","trusted":true},"cell_type":"code","source":"def get_angles(pos, i, d_model):\n  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n  return pos * angle_rates","execution_count":null,"outputs":[]},{"metadata":{"id":"xxFYxFOo55M1","trusted":true},"cell_type":"code","source":"def positional_encoding(position, d_model):\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n\n  # apply sin to even indices in the array; 2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n  # apply cos to odd indices in the array; 2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n  pos_encoding = angle_rads[np.newaxis, ...]\n\n  return tf.cast(pos_encoding, dtype=tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ZbzX60E58BU"},"cell_type":"markdown","source":"#### Look ahead maskÂ¶\n"},{"metadata":{"id":"zPGr-jHm5---","trusted":true},"cell_type":"code","source":"def create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask  # (seq_len, seq_len)","execution_count":null,"outputs":[]},{"metadata":{"id":"wEG9-yfd6B4z"},"cell_type":"markdown","source":"#### Scaled Dot Product Attention"},{"metadata":{"id":"YJQ-2NrS6FNc","trusted":true},"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"id":"hRn4GRyP6H19"},"cell_type":"markdown","source":"#### Multi Head Attention"},{"metadata":{"id":"gxrdILh26KPP","trusted":true},"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n\n    self.dense = tf.keras.layers.Dense(d_model)\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"id":"u8iz9DQL61_Q"},"cell_type":"markdown","source":"#### Pointwise FeedForward Network"},{"metadata":{"id":"jP7TSLvH65Ag","trusted":true},"cell_type":"code","source":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","execution_count":null,"outputs":[]},{"metadata":{"id":"JeudaHGQ67H6"},"cell_type":"markdown","source":"#### EncoderLayer"},{"metadata":{"id":"5bcO_5Y-6-oi","trusted":true},"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n\n  def call(self, x, training, mask):\n\n    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    attn_output = self.dropout1(attn_output, training=training)\n    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n    return out2","execution_count":null,"outputs":[]},{"metadata":{"id":"mbPkooHe7BRD"},"cell_type":"markdown","source":"#### Encoder"},{"metadata":{"id":"cGUaKNGc7Dy8","trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.content_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"content_id\"] + 2, d_model)\n    self.task_container_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"task_container_id\"] + 2, d_model)\n    self.part_emb = tf.keras.layers.Embedding(embed_size_dict[\"part\"] + 2, d_model)\n    self.prior_question_elapsed_time_emb = tf.keras.layers.Dense(d_model, use_bias=True)\n    self.prev_answered_emb = tf.keras.layers.Embedding(4, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                            self.d_model)\n\n\n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n\n    self.dropout = tf.keras.layers.Dropout(rate)\n\n  def call(self, x, training, mask):\n\n    seq_len = tf.shape(x)[1]\n\n    # adding embeddings and position encoding.\n    c_emb = self.content_id_emb(x[..., 0])  # (batch_size, input_seq_len, d_model)\n    t_emb = self.task_container_id_emb(x[..., 1])\n    prior_time_emb = self.prior_question_elapsed_time_emb(tf.expand_dims(x[..., 2], axis=-1))\n    pt_emb = self.part_emb(x[..., 3])\n    pv_emb = self.prev_answered_emb(x[..., 4])\n    x = c_emb + t_emb + prior_time_emb + pt_emb + pv_emb\n    \n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n\n    x = self.dropout(x, training=training)\n\n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x, training, mask) # (batch_size, input_seq_len, d_model)\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{"id":"vanS2PyP7Hgo","trusted":true},"cell_type":"code","source":"class TransformerSeq2SeqClassifier(keras.models.Model):\n  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate=0.1):\n    super(TransformerSeq2SeqClassifier, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate)\n    self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n\n  def call(self, x):\n    seq_len = tf.shape(x)[1]\n    look_ahead_mask = create_look_ahead_mask(seq_len)\n    encoded = self.encoder(x, mask=look_ahead_mask)\n\n    out = self.out(encoded)\n    return out # [batch_size, input_seq_len, 1]","execution_count":null,"outputs":[]},{"metadata":{"id":"6v2XHNJ77Jet"},"cell_type":"markdown","source":"#### Embedding sizes"},{"metadata":{"id":"0kDeBRYH7Q_8","trusted":true},"cell_type":"code","source":"# DATA_PATH = 'gs://kds-f48a9c4d95386273c0ef508e337abd3f874b82a454a6c3d0e035839c'\nDATA_PATH = \"/kaggle/input/riid-0-1\"","execution_count":null,"outputs":[]},{"metadata":{"id":"_poqM13u7RQd","outputId":"6f50b5c6-560c-4d34-ad86-ec247470a1ed","trusted":true},"cell_type":"code","source":"embed_sizes = pickle.loads(tf.io.read_file(DATA_PATH + \"/emb_sz.pkl\").numpy())\nembed_sizes","execution_count":null,"outputs":[]},{"metadata":{"id":"Q-3uU1WM7YDF","outputId":"59c0c7cb-6397-49fb-c849-f5bf98333bfd","trusted":true},"cell_type":"code","source":"model = TransformerSeq2SeqClassifier(\n      num_layers=1,\n      d_model=512,\n      num_heads=8,\n      dff=1024,\n      maximum_position_encoding=128,\n      embed_size_dict=embed_sizes\n  )\nmodel.build(input_shape=(128, 128, 5)) # input_shape - [batch_size, seq_len, features]\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"za2z4w357kEQ","trusted":true},"cell_type":"code","source":"model.load_weights(\"/kaggle/input/riid-model-2/best-model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"id":"F9Hmlvi870T4"},"cell_type":"markdown","source":"### Dataset"},{"metadata":{"id":"IuunfOds71fi"},"cell_type":"markdown","source":"#### Question df"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df = pd.read_csv(\"/kaggle/input/riiid-test-answer-prediction/questions.csv\", index_col=\"question_id\")","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test API"},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\n\n# You can only call make_env() once, so don't lose it!\nenv = riiideducation.make_env()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can only iterate through a result from `env.iter_test()` once\n# so be careful not to lose it once you start iterating.\niter_test = env.iter_test()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtypes_train = {\n    'row_id': 'int64',\n    'timestamp': 'int64',\n    'user_id': 'int32',\n    'content_id': 'int16',\n    'content_type_id': 'int8',\n    'task_container_id': 'int16',\n    'user_answer': 'int8',\n    'answered_correctly': 'int8',\n    'prior_question_elapsed_time': 'float32',\n    'prior_question_had_explanation': 'boolean'\n    }\n\ndtypes_questions = {\n    \"question_id\": \"\",\n    \"bundle_id\": \"\",\n    \"correct_answer\": \"\",\n    \"part\": \"int16\",\n    \"tags\": \"\",\n}","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 512","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submission"},{"metadata":{},"cell_type":"markdown","source":"#### Profile"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def time(func):\n#     def wrapped(*args, **kwargs):\n#         t0 = datetime.now()\n        \n#         ret = func(*args, **kwargs)\n        \n#         dt = datetime.now() - t0\n#         print(f\"==> {func.__name__} took {dt.total_seconds():.2} sec\")    \n#         return ret\n#     return wrapped\n\n# @time\ndef preprocess(test_dfm, prev_test_df):\n    \n    # we have prev df's answers, so store the prev_test_df in state_dict\n    if prev_test_df is not None:\n        try:\n            prev_test_df['answered_correctly'] = list(filter(lambda x: x !=-1, eval(test_dfm['prior_group_answers_correct'].iloc[0])))\n            prev_test_df['answered_correctly'] += 1\n            prev_user_group = prev_test_df.groupby(\"user_id\").apply(\n                lambda row: row.values[:, 1:] # exclude user_id\n            )\n            for user in prev_user_group.index:\n                if state_dict.get(user, None) is None: # new user, add to the state dict.\n                    state_dict[user] = prev_user_group[user]\n                else: # existing user\n                    state = np.vstack([state_dict[user], prev_user_group[user]]) # append to prev features.\n                    if state.shape[0] > SEQ_LEN-1: # we dont need history beyond SEQ_LEN-1\n                        state = state[-(SEQ_LEN-1):, :]\n                    state_dict[user] = state\n        except Exception as e:\n            pass # do not update the state dict.\n        \n    # dummy \"answered_correctly\" - fill token\n    test_dfm[\"answered_correctly\"] = 2\n    \n    # join question for feaures\n    test_df = test_dfm.join(questions_df, on=\"content_id\")\n    \n    # -- process the current df.\n    # selecting required cols\n    test_df = test_df[[\"user_id\", \"timestamp\", \"content_id\", \"task_container_id\", \"prior_question_elapsed_time\", \"prior_question_had_explanation\", \"part\", \"tags\", \"answered_correctly\"]]\n    \n    # 0, 1, 2 are special tokens, so increment 3\n    indicator_cols = [\"content_id\", \"task_container_id\", \"part\", \"prior_question_had_explanation\"]\n    for c in indicator_cols:\n      test_df[c] = test_df[c] + 3\n        \n    # same treatment for the tags\n    test_df[\"tags\"] = test_df[\"tags\"].apply(lambda row: \" \".join([str(int(x)+3) for x in row.split(\" \")]))\n    \n    # FIXME: unseen ids - content_id > 13525, 'task_container_id'> 10002, 'part'> 10\n    test_df.loc[test_df[\"content_id\"] > 13525, \"content_id\"] = 0\n    test_df.loc[test_df[\"task_container_id\"] > 10002, \"task_container_id\"] = 0\n    test_df.loc[test_df[\"part\"] > 10, \"part\"] = 0\n    # test_df.fillna(2, inplace=True) # FILL TOKEN\n    \n    return test_df\n\n# @time\ndef get_x(test_df):\n    max_seq_len = 1\n    xb = []\n    for row in test_df.values:\n        user = row[0]\n        x = np.zeros((SEQ_LEN+1, 8))\n        if state_dict.get(user, None) is None: # new user\n            x[-2, :] = [1] * 8 # start token\n            x[-1, :] = row[1:] # [1:] - skip user_id\n        else: # existing user, get prev states and build time series.\n            prev_state = state_dict[user]\n            row = np.append(row, 3) # this will be rolled to the first pos\n            curr_state = np.vstack([prev_state, row[1:]]) # [1:] - skip user_id\n\n            seq_len = curr_state.shape[0]\n            max_seq_len = max(seq_len, max_seq_len)\n\n            x[-seq_len:, :-1] = curr_state[:, :-1] # everything except prev_answer\n            x[-seq_len:, -1] = np.roll(curr_state[:, -1], shift=1) # rolled answer as prev_answer\n        xb.append(x)\n    x = np.stack(xb, axis=0)[:, -max_seq_len:, :]\n    return x\n\n# @time\ndef predict(x):\n    \n    # predict\n    preds = model(x, training=False)[:, -1, :].numpy().flatten()\n    # preds = model.predict(x)[:, -1, :].flatten()\n    \n    return preds\n\nstart = datetime.now()\n# test_dfs = [] # for debug\n\nprev_test_df = None\n\n# Loading training user states\nstate_dict = {}\n# with open(\"/kaggle/input/riid-state-dict-512-0-1/state_dict.pkl\", \"rb\") as f:\n#     state_dict = pickle.load(f)\n\nfor test_dfm, sample_pred in iter_test:\n#     print(\"\\n*\\n*\\n--- iteration ---\")\n#     t0 = datetime.now()\n    \n    # filtering only questions, removing lectures.\n    test_dfm = test_dfm[test_dfm.content_type_id == False]\n    test_df = preprocess(test_dfm, prev_test_df)\n    break\n#     test_dfs.append(test_df.copy())\n    \n    prev_test_df = test_df.copy()\n    \n    x = get_x(test_df)\n    \n    final_preds = predict(x)\n    \n    \n    # submit\n    test_dfm[\"answered_correctly\"] = final_preds\n    env.predict(test_dfm[[\"row_id\", \"answered_correctly\"]])\n#     print(f\"iteration took: {(datetime.now() - t0).total_seconds():.3}s\")\n    \ntotal_time = datetime.now() - start\nprint(f\"===== Total time: {total_time.total_seconds():.3} sec\")","execution_count":7,"outputs":[{"output_type":"stream","text":"===== Total time: 0.064 sec\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dfm","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"           row_id    timestamp     user_id  content_id  content_type_id  \\\ngroup_num                                                                 \n0               0            0   275030867        5729                0   \n0               1  13309898705   554169193       12010                0   \n0               2   4213672059  1720860329         457                0   \n0               3  62798072960   288641214       13262                0   \n0               4  10585422061  1728340777        6119                0   \n0               5  18020362258  1364159702       12023                0   \n0               6   2325432079  1521618396         574                0   \n0               7  39456940781  1317245193       12043                0   \n0               8   3460555189  1700555100        7910                0   \n0               9   2214770464   998511398        7908                0   \n0              10    516803182  1422853669        1143                0   \n0              11   2153839851  1096784725       11033                0   \n0              12   2153839851  1096784725       11032                0   \n0              13   2153839851  1096784725       11034                0   \n0              14   2153839851  1096784725       11031                0   \n0              15   1218852591   385471210        9538                0   \n0              16  32722340115  1202386221        1002                0   \n0              17   2059097926  2018567473       12148                0   \n\n           task_container_id  prior_question_elapsed_time  \\\ngroup_num                                                   \n0                          0                          NaN   \n0                       4427                      19000.0   \n0                        240                      17000.0   \n0                        266                      23000.0   \n0                        162                      72400.0   \n0                       4424                      18000.0   \n0                       1367                      18000.0   \n0                       5314                      17000.0   \n0                        532                      21000.0   \n0                        393                      21000.0   \n0                         85                      15000.0   \n0                        315                      34250.0   \n0                        315                      34250.0   \n0                        315                      34250.0   \n0                        315                      34250.0   \n0                        378                      11000.0   \n0                        136                      16000.0   \n0                        589                      17000.0   \n\n           prior_question_had_explanation prior_group_answers_correct  \\\ngroup_num                                                               \n0                                    <NA>                          []   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n0                                    True                         NaN   \n\n          prior_group_responses  \ngroup_num                        \n0                            []  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  \n0                           NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>timestamp</th>\n      <th>user_id</th>\n      <th>content_id</th>\n      <th>content_type_id</th>\n      <th>task_container_id</th>\n      <th>prior_question_elapsed_time</th>\n      <th>prior_question_had_explanation</th>\n      <th>prior_group_answers_correct</th>\n      <th>prior_group_responses</th>\n    </tr>\n    <tr>\n      <th>group_num</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>275030867</td>\n      <td>5729</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>13309898705</td>\n      <td>554169193</td>\n      <td>12010</td>\n      <td>0</td>\n      <td>4427</td>\n      <td>19000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>4213672059</td>\n      <td>1720860329</td>\n      <td>457</td>\n      <td>0</td>\n      <td>240</td>\n      <td>17000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>62798072960</td>\n      <td>288641214</td>\n      <td>13262</td>\n      <td>0</td>\n      <td>266</td>\n      <td>23000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>10585422061</td>\n      <td>1728340777</td>\n      <td>6119</td>\n      <td>0</td>\n      <td>162</td>\n      <td>72400.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>18020362258</td>\n      <td>1364159702</td>\n      <td>12023</td>\n      <td>0</td>\n      <td>4424</td>\n      <td>18000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>2325432079</td>\n      <td>1521618396</td>\n      <td>574</td>\n      <td>0</td>\n      <td>1367</td>\n      <td>18000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>39456940781</td>\n      <td>1317245193</td>\n      <td>12043</td>\n      <td>0</td>\n      <td>5314</td>\n      <td>17000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>3460555189</td>\n      <td>1700555100</td>\n      <td>7910</td>\n      <td>0</td>\n      <td>532</td>\n      <td>21000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>2214770464</td>\n      <td>998511398</td>\n      <td>7908</td>\n      <td>0</td>\n      <td>393</td>\n      <td>21000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>516803182</td>\n      <td>1422853669</td>\n      <td>1143</td>\n      <td>0</td>\n      <td>85</td>\n      <td>15000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>2153839851</td>\n      <td>1096784725</td>\n      <td>11033</td>\n      <td>0</td>\n      <td>315</td>\n      <td>34250.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>12</td>\n      <td>2153839851</td>\n      <td>1096784725</td>\n      <td>11032</td>\n      <td>0</td>\n      <td>315</td>\n      <td>34250.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>13</td>\n      <td>2153839851</td>\n      <td>1096784725</td>\n      <td>11034</td>\n      <td>0</td>\n      <td>315</td>\n      <td>34250.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>14</td>\n      <td>2153839851</td>\n      <td>1096784725</td>\n      <td>11031</td>\n      <td>0</td>\n      <td>315</td>\n      <td>34250.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>1218852591</td>\n      <td>385471210</td>\n      <td>9538</td>\n      <td>0</td>\n      <td>378</td>\n      <td>11000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>16</td>\n      <td>32722340115</td>\n      <td>1202386221</td>\n      <td>1002</td>\n      <td>0</td>\n      <td>136</td>\n      <td>16000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>2059097926</td>\n      <td>2018567473</td>\n      <td>12148</td>\n      <td>0</td>\n      <td>589</td>\n      <td>17000.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"              user_id    timestamp  content_id  task_container_id  \\\ngroup_num                                                           \n0           275030867            0        5732                  3   \n0           554169193  13309898705       12013               4430   \n0          1720860329   4213672059         460                243   \n0           288641214  62798072960       13265                269   \n0          1728340777  10585422061        6122                165   \n0          1364159702  18020362258       12026               4427   \n0          1521618396   2325432079         577               1370   \n0          1317245193  39456940781       12046               5317   \n0          1700555100   3460555189        7913                535   \n0           998511398   2214770464        7911                396   \n0          1422853669    516803182        1146                 88   \n0          1096784725   2153839851       11036                318   \n0          1096784725   2153839851       11035                318   \n0          1096784725   2153839851       11037                318   \n0          1096784725   2153839851       11034                318   \n0           385471210   1218852591        9541                381   \n0          1202386221  32722340115        1005                139   \n0          2018567473   2059097926       12151                592   \n\n           prior_question_elapsed_time  prior_question_had_explanation  part  \\\ngroup_num                                                                      \n0                                  NaN                            <NA>     8   \n0                              19000.0                               4     5   \n0                              17000.0                               4     5   \n0                              23000.0                               4     8   \n0                              72400.0                               4     8   \n0                              18000.0                               4     5   \n0                              18000.0                               4     5   \n0                              17000.0                               4     5   \n0                              21000.0                               4     4   \n0                              21000.0                               4     4   \n0                              15000.0                               4     5   \n0                              34250.0                               4     9   \n0                              34250.0                               4     9   \n0                              34250.0                               4     9   \n0                              34250.0                               4     9   \n0                              11000.0                               4     8   \n0                              16000.0                               4     5   \n0                              17000.0                               4     5   \n\n                    tags  \ngroup_num                 \n0                     11  \n0           93 103 95 32  \n0          146 108 41 32  \n0                     99  \n0                     57  \n0            5 110 95 32  \n0          146 144 41 32  \n0          146 117 41 32  \n0              13 181 95  \n0              134 96 41  \n0            5 110 41 95  \n0                     94  \n0                     30  \n0                    182  \n0                     46  \n0                      4  \n0           140 91 41 84  \n0           141 44 41 32  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>timestamp</th>\n      <th>content_id</th>\n      <th>task_container_id</th>\n      <th>prior_question_elapsed_time</th>\n      <th>prior_question_had_explanation</th>\n      <th>part</th>\n      <th>tags</th>\n    </tr>\n    <tr>\n      <th>group_num</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>275030867</td>\n      <td>0</td>\n      <td>5732</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>8</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>554169193</td>\n      <td>13309898705</td>\n      <td>12013</td>\n      <td>4430</td>\n      <td>19000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>93 103 95 32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1720860329</td>\n      <td>4213672059</td>\n      <td>460</td>\n      <td>243</td>\n      <td>17000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>146 108 41 32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>288641214</td>\n      <td>62798072960</td>\n      <td>13265</td>\n      <td>269</td>\n      <td>23000.0</td>\n      <td>4</td>\n      <td>8</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1728340777</td>\n      <td>10585422061</td>\n      <td>6122</td>\n      <td>165</td>\n      <td>72400.0</td>\n      <td>4</td>\n      <td>8</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1364159702</td>\n      <td>18020362258</td>\n      <td>12026</td>\n      <td>4427</td>\n      <td>18000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5 110 95 32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1521618396</td>\n      <td>2325432079</td>\n      <td>577</td>\n      <td>1370</td>\n      <td>18000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>146 144 41 32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1317245193</td>\n      <td>39456940781</td>\n      <td>12046</td>\n      <td>5317</td>\n      <td>17000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>146 117 41 32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1700555100</td>\n      <td>3460555189</td>\n      <td>7913</td>\n      <td>535</td>\n      <td>21000.0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>13 181 95</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>998511398</td>\n      <td>2214770464</td>\n      <td>7911</td>\n      <td>396</td>\n      <td>21000.0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>134 96 41</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1422853669</td>\n      <td>516803182</td>\n      <td>1146</td>\n      <td>88</td>\n      <td>15000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5 110 41 95</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1096784725</td>\n      <td>2153839851</td>\n      <td>11036</td>\n      <td>318</td>\n      <td>34250.0</td>\n      <td>4</td>\n      <td>9</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1096784725</td>\n      <td>2153839851</td>\n      <td>11035</td>\n      <td>318</td>\n      <td>34250.0</td>\n      <td>4</td>\n      <td>9</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1096784725</td>\n      <td>2153839851</td>\n      <td>11037</td>\n      <td>318</td>\n      <td>34250.0</td>\n      <td>4</td>\n      <td>9</td>\n      <td>182</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1096784725</td>\n      <td>2153839851</td>\n      <td>11034</td>\n      <td>318</td>\n      <td>34250.0</td>\n      <td>4</td>\n      <td>9</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>385471210</td>\n      <td>1218852591</td>\n      <td>9541</td>\n      <td>381</td>\n      <td>11000.0</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1202386221</td>\n      <td>32722340115</td>\n      <td>1005</td>\n      <td>139</td>\n      <td>16000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>140 91 41 84</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2018567473</td>\n      <td>2059097926</td>\n      <td>12151</td>\n      <td>592</td>\n      <td>17000.0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>141 44 41 32</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Done."},{"metadata":{"trusted":true},"cell_type":"code","source":"int64_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int64_feat.npz\").todense()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int32_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int32_feat.npz\").todense()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int16_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int16_feat.npz\").todense()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int8_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int8_feat.npz\").todense()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_index = dict()\nwith open(\"/kaggle/input/riid-state-dict-512-0-7/user_index.pkl\", \"rb\") as f:\n    user_index = pickle.load(f)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/tags_feat.npz\")","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_feat[user_index[115]].todense()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}