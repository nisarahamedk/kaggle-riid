{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RIID TF Transformers TPU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c0cfe4ac88b4f7eb9bbe140fcc5524b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08ac3215becd491a97f2bbe4f9f8f54b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d98106bb7ab44a27a922f2a250d999f5",
              "IPY_MODEL_3a1fc395589048ba8c451ab966d0b848"
            ]
          }
        },
        "08ac3215becd491a97f2bbe4f9f8f54b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d98106bb7ab44a27a922f2a250d999f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_8d8dc44d56134088a171f951f569277c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 121.25MB of 121.25MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f2f248b8aec4f009adf9133e532250f"
          }
        },
        "3a1fc395589048ba8c451ab966d0b848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_45e6e05f363343a9bf4f4336f086f744",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b66a376faef24a6280ab43ffb3088a25"
          }
        },
        "8d8dc44d56134088a171f951f569277c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f2f248b8aec4f009adf9133e532250f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45e6e05f363343a9bf4f4336f086f744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b66a376faef24a6280ab43ffb3088a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisarahamedk/kaggle-riid/blob/master/notebooks/RIID_TF_EncoderDecoder_Transformers_TPU_WandB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcEKftRzz3o"
      },
      "source": [
        "#### RIID Transformer on TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FJ7eTHbRHbD"
      },
      "source": [
        "%%capture\r\n",
        "!pip install watermark\r\n",
        "\r\n",
        "!pip uninstall -y wandb\r\n",
        "!pip uninstall -y numpy\r\n",
        "!pip uninstall -y pandas\r\n",
        "!pip uninstall -y tensorflow\r\n",
        "\r\n",
        "!pip install --upgrade wandb==0.10.12\r\n",
        "!pip install --upgrade pandas==1.1.5\r\n",
        "!pip install --upgrade numpy==1.19.4\r\n",
        "!pip install --upgrade tensorflow==2.4.0\r\n",
        "!pip install cloud_tpu_client\r\n",
        "\r\n",
        "# In Kaggle, always restart the kernel after installing libs.\r\n",
        "# With the focus on a cell, press Ctrl+Shift+P and choose \"confirm restart kernel\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-tXEAUsz-e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa9c529-47be-4401-e8a2-25279a259fa4"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%reload_ext watermark\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.python.autograph.impl import api as autograph\n",
        "from tensorflow.python.autograph.core import ag_ctx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from cloud_tpu_client import Client\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "%watermark -iv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow.keras 2.4.0\n",
            "tensorflow       2.4.0\n",
            "wandb            0.10.12\n",
            "numpy            1.19.4\n",
            "pandas           1.1.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZwdiZXG0XgB"
      },
      "source": [
        "\n",
        "### *Device* Settings - This needs to be at the TOP¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhFY3xYNVKfG"
      },
      "source": [
        "Client().configure_tpu_version(tf.__version__, restart_type='ifNeeded')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3giJ9Nu0aTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe795677-6ec8-4b30-e55f-0f138e5e9f39"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # no parameter needed for TPU_NAME env variable is set. This is the case for Kaggle\n",
        "    print(\"Running on TPU: \", tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU:  grpc://10.97.81.98:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8cViVXg0dRt",
        "outputId": "2712911c-8256-48c6-8865-1e52205cd097"
      },
      "source": [
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    # default strategy with the available hw\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    \n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print(\"REPLICAS: \", REPLICAS)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.97.81.98:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.97.81.98:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlZu4XIZRnjp"
      },
      "source": [
        "#### WandB Experiment Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LurQJR9sUGIo"
      },
      "source": [
        "resume = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbXfr7SSRqw-"
      },
      "source": [
        "experiment_config = {\r\n",
        "    \"dataset_args\": {\r\n",
        "        \"tfrec_gcs_path\": 'gs://kds-9c9a89c1a0d17bcb15230bb2e47c4641386843a62d12638c765e35cb',\r\n",
        "        \"folds\": 10,\r\n",
        "        \"fold\": 1,\r\n",
        "        \"batch_size\": 256,\r\n",
        "        \"seq_len\": 512,\r\n",
        "    },\r\n",
        "    \"model_args\": {\r\n",
        "        \"num_layers\": 1,\r\n",
        "        \"d_model\": 512,\r\n",
        "        \"num_heads\": 8,\r\n",
        "        \"dff\": 1024,\r\n",
        "    },\r\n",
        "    \"training_args\": {\r\n",
        "        \"epochs\": 30,\r\n",
        "    },\r\n",
        "    \"env_args\": {\r\n",
        "        \"is_kaggle\": False,\r\n",
        "        \"upload_model_to_kaggle\": False\r\n",
        "    }\r\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwhqiBPjT-m8",
        "outputId": "83df5459-a813-42d6-b4e0-39260e891959"
      },
      "source": [
        "!wandb login f137298421da563b24639d1287dd3ce5da537814"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLpax_ixUBAy"
      },
      "source": [
        "notes = \"test run\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "s_KrLPRyUCLI",
        "outputId": "36c4b25a-1f41-4d77-bb89-5c7b7b08fa5a"
      },
      "source": [
        "wandb_run = wandb.init(project=\"kaggle-riid\", notes=notes, resume=resume)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnisarahamedk\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">autumn-gorge-27</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nisarahamedk/kaggle-riid\" target=\"_blank\">https://wandb.ai/nisarahamedk/kaggle-riid</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/nisarahamedk/kaggle-riid/runs/3sz12zm2\" target=\"_blank\">https://wandb.ai/nisarahamedk/kaggle-riid/runs/3sz12zm2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201230_055544-3sz12zm2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-sjdjBuUPS_",
        "outputId": "1e84ea4e-1eed-4dd7-9fed-e03830277c2a"
      },
      "source": [
        "api = wandb.Api()\r\n",
        "run = api.run(\"nisarahamedk/kaggle-riid/\" + wandb.run.id)\r\n",
        "run"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Run nisarahamedk/kaggle-riid/3sz12zm2 (running)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_wyyDqCUVMr"
      },
      "source": [
        "if resume:\r\n",
        "  experiment_config = run.config"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHATf5SI0f7k"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZyQj3J-0sCh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bac421dc-f17f-4293-bbac-dc72f0d6ea42"
      },
      "source": [
        "DATA_PATH = experiment_config[\"dataset_args\"][\"tfrec_gcs_path\"]\r\n",
        "DATA_PATH"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://kds-9c9a89c1a0d17bcb15230bb2e47c4641386843a62d12638c765e35cb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNldoAXn1GOf",
        "outputId": "2ffb2f4f-11d4-49c4-ec98-13cfd105086d"
      },
      "source": [
        "n_train_files = len(tf.io.gfile.glob(DATA_PATH + \"/tfrec*\"))\n",
        "n_train_files"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KLXao8j1jSp",
        "outputId": "78435ecf-0ebf-4d60-ac1d-ddfd3cc39064"
      },
      "source": [
        "FOLDS = experiment_config[\"dataset_args\"][\"folds\"]\n",
        "\n",
        "kfold = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "folds_list = list(kfold.split(np.arange(n_train_files)))\n",
        "folds_list[:2]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 18,\n",
              "         19, 20, 21, 22, 23, 25, 26, 27, 28, 30, 31]),\n",
              "  array([15, 17, 24, 29])),\n",
              " (array([ 0,  1,  2,  3,  4,  5,  6,  7, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 31]),\n",
              "  array([ 8,  9, 25, 30]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC8RRFAM11rq",
        "outputId": "578555d2-15fa-49ee-ca31-de129d44d566"
      },
      "source": [
        "FOLD = experiment_config[\"dataset_args\"][\"fold\"]\n",
        "\n",
        "train_folds, valid_folds = folds_list[FOLD]\n",
        "len(train_folds), len(valid_folds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Myj8kbWck4"
      },
      "source": [
        "experiment_config[\"dataset_args\"].update({\"train_folds\": str(list(train_folds)), \"valid_folds\": str(list(valid_folds))})"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztufELqg2Xfn"
      },
      "source": [
        "train_files = tf.io.gfile.glob([DATA_PATH + \"/tfrec_%d.tfrec\" % idx for idx in train_folds])\n",
        "valid_files = tf.io.gfile.glob([DATA_PATH + \"/tfrec_%d.tfrec\" % idx for idx in valid_folds])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf6yVysK22E6",
        "outputId": "18d3b54b-1441-4b95-9737-2a64673fffbb"
      },
      "source": [
        "len(train_files), len(valid_files)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHpRgO2L25aj"
      },
      "source": [
        "#### Load TFRecord Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0zEw3u23jmr"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrXHOsZ13sEX"
      },
      "source": [
        "feature_desc = {\n",
        "    \"timestamp\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"content_id\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"task_container_id\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"elapsed_time\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"had_explanation\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"part\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"tags\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"answered_correctly\": tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "def parse_example(example):\n",
        "  example = tf.io.parse_single_example(example, feature_desc)\n",
        "\n",
        "  timestamp = tf.io.parse_tensor(example[\"timestamp\"], tf.int64)\n",
        "  content_id = tf.io.parse_tensor(example[\"content_id\"], tf.int16)\n",
        "  task_container_id = tf.io.parse_tensor(example[\"task_container_id\"], tf.int16)\n",
        "  elapsed_time = tf.io.parse_tensor(example[\"elapsed_time\"], tf.float32)\n",
        "  had_explanation = tf.io.parse_tensor(example[\"had_explanation\"], tf.int8)\n",
        "  part = tf.io.parse_tensor(example[\"part\"], tf.int16)\n",
        "  answered_correctly = tf.io.parse_tensor(example[\"answered_correctly\"], tf.int8)\n",
        "\n",
        "  # tags\n",
        "  tags = tf.io.parse_tensor(example[\"tags\"], tf.string) # as string, one q can have multiple tags.\n",
        "  tags = tf.strings.to_number(tf.strings.split(tags), out_type=tf.int32) # will produce a ragged tensor with tags for each q\n",
        "  # ragged tensor of tags [[2], [3, 4]] is converted to one hot like [[0,0,1..], [[0,0,0,1,..], [0,0,0,0,1...]]]\n",
        "  # then sumed along axis 1, so for each question there will be 1 for all the tags associated with it.\n",
        "  tags = tf.reduce_sum(tf.one_hot(tags, depth=190), axis=1) # shape [seq_len, 190]\n",
        "  tags = tf.transpose(tags, (1, 0)) # shape [190, seq_len] to stack with other features.\n",
        "  \n",
        "  features = tf.stack([\n",
        "      tf.cast(timestamp, tf.float32),\n",
        "      tf.cast(content_id, tf.float32),\n",
        "      tf.cast(task_container_id, tf.float32),\n",
        "      tf.cast(elapsed_time, tf.float32),\n",
        "      tf.cast(had_explanation, tf.float32),\n",
        "      tf.cast(part, tf.float32),\n",
        "      tf.cast(answered_correctly, tf.float32),\n",
        "  ])\n",
        "\n",
        "  # add tags\n",
        "  return tf.concat([\n",
        "      features,\n",
        "      tf.cast(tags, tf.float32),\n",
        "  ], axis=0) # [features, seq_len] # TODO make it [seq_len, features] so that we dont have to reshape in transformer"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1vZigwi3QVa"
      },
      "source": [
        "def load_dataset_from_tfrecord(filenames, ds_type=\"train\", cache_to=None):\n",
        "        # Since we are reading dataset from multiple files. and we dont care about the order.\n",
        "        # set deterministic reading to False.\n",
        "        ignore_order = tf.data.Options()\n",
        "        if ds_type == \"train\":\n",
        "            ignore_order.experimental_deterministic = False\n",
        "            \n",
        "        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
        "        if not cache_to:\n",
        "            dataset = dataset.cache() # cache to RAM\n",
        "        else:\n",
        "            dataset = dataset.cache(cache_to) # cache to file given by self.cache_to \n",
        "        if ds_type == \"train\":\n",
        "            # dataset = dataset.repeat() # repeat individual item, so that we have full batch at every step.\n",
        "            pass\n",
        "        dataset.with_options(ignore_order)\n",
        "        dataset = dataset.map(parse_example, num_parallel_calls=AUTOTUNE)\n",
        "        return dataset"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhaJdlb537yQ"
      },
      "source": [
        "dataset = load_dataset_from_tfrecord(train_files)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JPG7fOy4jCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcdec71-9b39-4849-d729-f6d6332dab9b"
      },
      "source": [
        "SEQ_LEN = experiment_config[\"dataset_args\"][\"seq_len\"]\r\n",
        "SEQ_LEN"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4QnfOgm4FWT"
      },
      "source": [
        "@tf.function\n",
        "def pad(a, seq_len, max_seq_len):\n",
        "  s = max_seq_len - seq_len\n",
        "  # making [[0, 0], [s, 0]]\n",
        "  r = tf.stack([s, tf.constant(0)])\n",
        "  t = tf.stack([tf.constant([0, 0]), r])\n",
        "  \n",
        "  return tf.pad(a, t) # ,1 to debug\n",
        "\n",
        "@tf.function\n",
        "def trim(a, seq_len,  max_seq_len):\n",
        "  \"\"\"\n",
        "  TODO: trimming actually get rid of the start token since we are trimming randomly in the model.\n",
        "  Is this going to be a problem?, i think it should not.\n",
        "  \"\"\"\n",
        "  start = tf.squeeze(tf.random.uniform((1,), maxval=(seq_len-max_seq_len), dtype=tf.int32))\n",
        "  # https://www.quora.com/How-does-tf-slice-work-in-TensorFlow\n",
        "  begin = tf.stack([tf.constant(0), start])\n",
        "  size = tf.stack([tf.shape(a)[0], max_seq_len])\n",
        "  \n",
        "  return tf.slice(a, begin, size) # , start - to debug\n",
        "\n",
        "@tf.function\n",
        "def pad_or_trim(a):\n",
        "  seq_len = tf.shape(a)[-1]\n",
        "  max_seq_len = SEQ_LEN + 1 # accomodate for the start token\n",
        "  fn = tf.cond(tf.less_equal(seq_len, max_seq_len), lambda: pad(a, seq_len, max_seq_len), lambda: trim(a, seq_len, max_seq_len))\n",
        "  return fn"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi2ty0Rd4kyp"
      },
      "source": [
        "dataset = dataset.map(pad_or_trim, num_parallel_calls=AUTOTUNE) # every sample is padded if len < SEQ_LEN or randomly trimmed to SEQ_LEN"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p01N7j3IPV6o",
        "outputId": "6bebe6d9-12c0-4a27-a623-c34ad258cc47"
      },
      "source": [
        "for item in dataset.take(1):\r\n",
        "  print(item.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(197, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOTG_iFxa5gc"
      },
      "source": [
        "@tf.function\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.reduce_all(tf.math.equal(seq, 0), axis=-1), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "@tf.function\n",
        "def split_x_y_mask(xy):\n",
        "  # x = xy[:-1, :] # we need 'answered_correctly' in x\n",
        "  x = tf.transpose(xy, (1, 0)) # [seq_len, n_features]\n",
        "  y = xy[6, :]\n",
        "  y = y - 3 # +3 had added to use this in feature, make it back to 0,1\n",
        "  pad_mask = tf.cast(tf.math.reduce_any(tf.math.not_equal(x, 0), axis=-1), dtype=tf.float32)\n",
        "  start_token_mask = tf.cast(tf.math.logical_not(tf.math.reduce_all(tf.math.equal(x[:, :7], 1), axis=-1)), dtype=tf.float32)\n",
        "  final_mask = tf.math.multiply(pad_mask, start_token_mask)\n",
        "  return x, tf.expand_dims(y[1:], axis=-1), tf.expand_dims(final_mask[1:], axis=-1) # [1:] for y and mask, because first val is for start token"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_CI26N0K3R6"
      },
      "source": [
        "dataset = dataset.map(split_x_y_mask, num_parallel_calls=AUTOTUNE) # x and y"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5eT_hwk4qaY",
        "outputId": "2934e426-4793-402c-8555-37813d787109"
      },
      "source": [
        "for x, y, mask in dataset.take(1):\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(mask.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(513, 197)\n",
            "(512, 1)\n",
            "(512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS0Jw9j1hxTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64691695-b0fa-4721-e78a-b6d9c3e385f0"
      },
      "source": [
        "tf.squeeze(mask, axis=-1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE8uOJbx2W2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca952964-c6e4-4ae0-a911-1a156104047b"
      },
      "source": [
        "tf.squeeze(y, axis=-1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([-3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
              "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -2.,  1.,  1.,\n",
              "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,\n",
              "        1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
              "        1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
              "        1.,  0.,  0.,  0.,  1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rUcga1etGfe",
        "outputId": "4f95bb49-39f5-4b39-e637-700c74edf0b0"
      },
      "source": [
        "tf.squeeze(y, axis=-1) * tf.squeeze(mask, axis=-1) #  applying mask"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
              "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  1.,  1.,\n",
              "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,\n",
              "        1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
              "        1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
              "        1.,  0.,  0.,  0.,  1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVuc7KqWFZ01"
      },
      "source": [
        "dataset = dataset.shuffle(int(1024 * REPLICAS))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX3iCpAnFvIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f60c47-965a-4c49-be29-ac5d9b2c2d84"
      },
      "source": [
        "BATCH_SIZE = experiment_config[\"dataset_args\"][\"batch_size\"]\r\n",
        "BATCH_SIZE"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp8zSQyaFzLV"
      },
      "source": [
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMQdN2zgF9hw",
        "outputId": "57daf795-9276-4870-b5d1-cee171c008c3"
      },
      "source": [
        "for xb, yb, mb in dataset.take(1):\n",
        "  print(xb.shape)\n",
        "  print(yb.shape)\n",
        "  print(mb.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 513, 197)\n",
            "(256, 512, 1)\n",
            "(256, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEpW8ID94dPC",
        "outputId": "14c5e4d2-b542-4f88-e4cb-3d60fe75e021"
      },
      "source": [
        "# answered_correctly, # notice for trimmed sequence, there is no start token, instead a previous interaction is used.\n",
        "xb[0, :, 6]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(513,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 4., 3., 3., 4., 4., 3., 4., 3., 3., 3.,\n",
              "       3., 3., 3., 3., 3., 4., 3., 3., 3., 3., 3., 3., 4., 3., 3., 3., 3.,\n",
              "       3., 3., 3.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S33r8rmn4kdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf97439-c643-4a6c-ea79-a71e6cd2e69f"
      },
      "source": [
        "# corresponding mask\n",
        "mb[0, :, 0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbmVLJKDGBYX"
      },
      "source": [
        "dataset = dataset.prefetch(AUTOTUNE)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIKTPaHerpwF"
      },
      "source": [
        "Valid dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCL1p1-uruwd"
      },
      "source": [
        "valid_dataset = load_dataset_from_tfrecord(valid_files, ds_type=\"valid\")\n",
        "valid_dataset = valid_dataset.map(pad_or_trim, num_parallel_calls=AUTOTUNE) # every sample is padded if len < SEQ_LEN or randomly trimmed to SEQ_LEN\n",
        "valid_dataset = valid_dataset.map(split_x_y_mask) # x, y and mask\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE * 2, drop_remainder=True)\n",
        "valid_dataset = valid_dataset.prefetch(AUTOTUNE)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYZJpTSrTf0h",
        "outputId": "a8130d72-d193-485a-c8b0-54ae3ec6f837"
      },
      "source": [
        "for vx, vy, vmask in valid_dataset.take(1):\r\n",
        "  print(vx.shape)\r\n",
        "  print(vy.shape)\r\n",
        "  print(vmask.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(512, 513, 197)\n",
            "(512, 512, 1)\n",
            "(512, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-l6VS3iTtRw",
        "outputId": "2716a8a0-6e7b-466c-958c-44fc1025665b"
      },
      "source": [
        "# answered_correctly\r\n",
        "vx[0, :, 6]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(513,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 4., 4., 4., 4., 3., 4., 4., 4., 4., 3.,\n",
              "       4., 4., 4., 4., 4., 4., 3., 4., 4., 3., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJFEWh7AT2Mc",
        "outputId": "fb4b7923-c5a1-4d9c-a0be-b6836922cbde"
      },
      "source": [
        "# corresponding mask\r\n",
        "vmask[0, :, 0]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY6reftTtmY1",
        "outputId": "39c446b7-6967-47dc-b64b-0196ef143df5"
      },
      "source": [
        "vx[0, 1:, 6] * vmask[0, :, 0] # checking the mask"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 4., 4., 4., 4., 3., 4., 4., 4., 4., 3., 4.,\n",
              "       4., 4., 4., 4., 4., 3., 4., 4., 3., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7N2qkPoGF8n"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzidzjELLqY7"
      },
      "source": [
        "##### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3OWpbddMhgh"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA17TuuTMh8R"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pROxdYkMkRi"
      },
      "source": [
        "##### Look ahead mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ZVSk3oMxg3"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfDhTLswMyDl"
      },
      "source": [
        "##### Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJhA4M7XM8zY"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak8Y8-ihNG1j"
      },
      "source": [
        "##### Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkL0KZ5uNMS7"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwQteAZBNPtG"
      },
      "source": [
        "##### Pointwise FeedForward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiYVEX_0NX54"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGbRm0LSNau_"
      },
      "source": [
        "##### EncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V96wCZDvNexl"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7i7zR9xNjMj"
      },
      "source": [
        "##### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM4bREj9Nrp_"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # timestamp embedding\n",
        "    self.timestamp_buckets = list(np.linspace(0, 87425772049, num=1000)) # close to 1 hour per bucket\n",
        "    self.timestamp_emb = tf.keras.layers.Embedding(len(self.timestamp_buckets) + 1, d_model)\n",
        "    # question id embedding\n",
        "    self.content_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"content_id\"] + 1, d_model)\n",
        "    # task container embedding\n",
        "    self.task_container_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"task_container_id\"] + 1, d_model)\n",
        "    # part embedding\n",
        "    self.part_emb = tf.keras.layers.Embedding(embed_size_dict[\"part\"] + 1, d_model)\n",
        "    # tags/skills embedding\n",
        "    self.tags_emb = tf.Variable(tf.random.uniform([embed_size_dict[\"tags\"], d_model]))\n",
        "    # elapsed_time embedding\n",
        "    self.elapsed_time_buckets = list(np.linspace(0, 300000, num=10)) # close to 30sec per bucket\n",
        "    self.elapsed_time_emb = tf.keras.layers.Embedding(len(self.elapsed_time_buckets) + 1, d_model)\n",
        "    # answered_correctly embedding\n",
        "    self.answered_correctly_emb = tf.keras.layers.Embedding(embed_size_dict[\"answered_correctly\"], d_model)\n",
        "    # position encoding\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, input, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(input)[1]\n",
        "\n",
        "    \"\"\"\n",
        "    tf.cast(timestamp, tf.float32),\n",
        "    tf.cast(content_id, tf.float32),\n",
        "    tf.cast(task_container_id, tf.float32),\n",
        "    tf.cast(elapsed_time, tf.float32),\n",
        "    tf.cast(had_explanation, tf.float32),\n",
        "    tf.cast(part, tf.float32),\n",
        "    tf.cast(answered_correctly, tf.float32),\n",
        "    \"\"\"\n",
        "    # adding embeddings and position encoding.\n",
        "    # --- question related embeddings\n",
        "    content_id_emb = self.content_id_emb(input[..., 1])  # (batch_size, input_seq_len, d_model)\n",
        "    x = content_id_emb\n",
        "    task_container_id_emb = self.task_container_id_emb(input[..., 2])\n",
        "    x += task_container_id_emb\n",
        "    part_emb = self.part_emb(input[..., 5])\n",
        "    x += part_emb\n",
        "    # tags/skills\n",
        "    tags_oh = input[:, :, 7:]\n",
        "    # tags_oh = tf.nn.softmax(tags_oh * 100) # soft max so that the weights adds up to 1, multiplied by 100 to make the non-skills to 0, reduce this if importance needs to be given to non-skills.\n",
        "    tags_emb = tf.linalg.matmul(tags_oh, self.tags_emb) # multiple skills can be there. this sum up the tag embeddings\n",
        "    x += tags_emb\n",
        "\n",
        "    # --- response related embeddings\n",
        "    timestamp_buckets = tf.raw_ops.Bucketize(input=input[..., 0], boundaries=self.timestamp_buckets)\n",
        "    timestamp_emb = self.timestamp_emb(timestamp_buckets)\n",
        "    x += timestamp_emb\n",
        "    elapsed_time_buckets = tf.raw_ops.Bucketize(input=input[..., 3], boundaries=self.elapsed_time_buckets)\n",
        "    elapsed_time_emb = self.elapsed_time_emb(elapsed_time_buckets)\n",
        "    x += elapsed_time_emb\n",
        "    answered_correctly_emb = self.answered_correctly_emb(input[..., 6])\n",
        "    x += answered_correctly_emb\n",
        "    \n",
        "    \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask) # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEjweChzd0PL"
      },
      "source": [
        "##### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXKW4K9fd2sj"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(DecoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\r\n",
        "\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        "\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "\r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, x, enc_output, training, look_ahead_mask):\r\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n",
        "\r\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn1 = self.dropout1(attn1, training=training)\r\n",
        "    out1 = self.layernorm1(attn1 + x)\r\n",
        "\r\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn2 = self.dropout2(attn2, training=training)\r\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\r\n",
        "\r\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\r\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "\r\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDyHNjP7d78P"
      },
      "source": [
        "##### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEPz4Said-xV"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "\r\n",
        "    self.content_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"content_id\"] + 1, d_model)\r\n",
        "    self.task_container_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"task_container_id\"] + 1, d_model)\r\n",
        "    self.part_emb = tf.keras.layers.Embedding(embed_size_dict[\"part\"] + 1, d_model)\r\n",
        "    self.tags_emb = tf.Variable(tf.random.uniform([embed_size_dict[\"tags\"], d_model]))\r\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\r\n",
        "\r\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "\r\n",
        "  def call(self, input, enc_output, training, look_ahead_mask):\r\n",
        "\r\n",
        "    seq_len = tf.shape(input)[1]\r\n",
        "    attention_weights = {}\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    tf.cast(timestamp, tf.float32),\r\n",
        "    tf.cast(content_id, tf.float32),\r\n",
        "    tf.cast(task_container_id, tf.float32),\r\n",
        "    tf.cast(elapsed_time, tf.float32),\r\n",
        "    tf.cast(had_explanation, tf.float32),\r\n",
        "    tf.cast(part, tf.float32),\r\n",
        "    tf.cast(answered_correctly, tf.float32),\r\n",
        "    \"\"\"\r\n",
        "    # adding embeddings and position encoding.\r\n",
        "    content_id_emb = self.content_id_emb(input[..., 1])  # (batch_size, input_seq_len, d_model)\r\n",
        "    x = content_id_emb\r\n",
        "    task_container_id_emb = self.task_container_id_emb(input[..., 2])\r\n",
        "    x += task_container_id_emb\r\n",
        "    part_emb = self.part_emb(input[..., 5])\r\n",
        "    x += part_emb\r\n",
        "    # tags/skills\r\n",
        "    tags_oh = input[:, :, 7:]\r\n",
        "    # tags_oh = tf.nn.softmax(tags_oh * 100) # soft max so that the weights adds up to 1, multiplied by 100 to make the non-skills to 0, reduce this if importance needs to be given to non-skills.\r\n",
        "    tags_emb = tf.linalg.matmul(tags_oh, self.tags_emb) # multiple skills can be there. this sum up the tag embeddings\r\n",
        "    x += tags_emb\r\n",
        "    \r\n",
        "\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    x += self.pos_encoding[:, :seq_len, :]\r\n",
        "\r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "\r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask)\r\n",
        "\r\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\r\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\r\n",
        "\r\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\r\n",
        "    return x, attention_weights"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PcxHfXTp64m"
      },
      "source": [
        "class TransformerSeq2SeqClassifier(keras.models.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate=0.1):\n",
        "    super(TransformerSeq2SeqClassifier, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate)\n",
        "    \n",
        "    self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, x, training):\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(SEQ_LEN)\n",
        "    \n",
        "    # encoder\n",
        "    enc_input = x[:, :SEQ_LEN, :]\n",
        "    enc_output = self.encoder(enc_input, training=training, mask=look_ahead_mask)\n",
        "\n",
        "    # decoder\n",
        "    dec_input = x[:, 1:, :]\n",
        "    dec_output, attention_weights = self.decoder(dec_input, enc_output, training=training, look_ahead_mask=look_ahead_mask)\n",
        "\n",
        "    out = self.out(dec_output)\n",
        "    return out # [batch_size, input_seq_len, 1]\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFiVa3cKN4E_"
      },
      "source": [
        "###### Embedding Sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xU0IaYQX_XG"
      },
      "source": [
        "embed_sizes = pickle.loads(tf.io.read_file(DATA_PATH + \"/emb_sz.pkl\").numpy())"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfN7FrvjY02x",
        "outputId": "9950efed-9fa0-409f-f013-c03009664920"
      },
      "source": [
        "embed_sizes"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answered_correctly': 5,\n",
              " 'content_id': 13525,\n",
              " 'had_explanation': 5,\n",
              " 'part': 10,\n",
              " 'tags': 190,\n",
              " 'task_container_id': 10002}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGZvmPR2UkH9"
      },
      "source": [
        "experiment_config[\"model_args\"].update({\"embed_sizes\": embed_sizes})"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VOLk9rgXo5X",
        "outputId": "4a95097c-f38f-4049-9b84-cfa0311f07b0"
      },
      "source": [
        "experiment_config"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset_args': {'batch_size': 256,\n",
              "  'fold': 1,\n",
              "  'folds': 10,\n",
              "  'seq_len': 512,\n",
              "  'tfrec_gcs_path': 'gs://kds-9c9a89c1a0d17bcb15230bb2e47c4641386843a62d12638c765e35cb',\n",
              "  'train_folds': '[0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 31]',\n",
              "  'valid_folds': '[8, 9, 25, 30]'},\n",
              " 'env_args': {'is_kaggle': False, 'upload_model_to_kaggle': False},\n",
              " 'model_args': {'d_model': 512,\n",
              "  'dff': 1024,\n",
              "  'embed_sizes': {'answered_correctly': 5,\n",
              "   'content_id': 13525,\n",
              "   'had_explanation': 5,\n",
              "   'part': 10,\n",
              "   'tags': 190,\n",
              "   'task_container_id': 10002},\n",
              "  'num_heads': 8,\n",
              "  'num_layers': 1},\n",
              " 'training_args': {'epochs': 30}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWvXJsP3Y2Bt"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = TransformerSeq2SeqClassifier(\n",
        "      num_layers=experiment_config[\"model_args\"][\"num_layers\"],\n",
        "      d_model=experiment_config[\"model_args\"][\"d_model\"],\n",
        "      num_heads=experiment_config[\"model_args\"][\"num_heads\"],\n",
        "      dff=experiment_config[\"model_args\"][\"dff\"],\n",
        "      maximum_position_encoding=SEQ_LEN,\n",
        "      embed_size_dict=embed_sizes\n",
        "  )"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "givnKZG-ar9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f501fc7-6957-49ff-c9d4-2362381df05e"
      },
      "source": [
        "for xb, yb, mb in dataset.take(1):\n",
        "  print(xb.shape, yb.shape, mb.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 513, 197) (256, 512, 1) (256, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubeynQDIa3V_"
      },
      "source": [
        "with strategy.scope():\r\n",
        "  y_pred = model(xb, training=False)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjFuHPZk8fa",
        "outputId": "463a04db-ba4f-400a-f886-16c21c77c4ec"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([256, 512, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCYG-IobCINv"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcWnSd8TpiZi"
      },
      "source": [
        "class CustomAUC(keras.metrics.Metric):\r\n",
        "\r\n",
        "  def __init__(self, **kwargs):\r\n",
        "    super().__init__(**kwargs)\r\n",
        "\r\n",
        "    self.auc = keras.metrics.AUC()\r\n",
        "\r\n",
        "  def update_state(self, y_true, y_pred, sample_weight):\r\n",
        "\r\n",
        "    self.auc.update_state(y_true, y_pred, sample_weight)\r\n",
        "\r\n",
        "  def result(self):\r\n",
        "    return self.auc.result()\r\n",
        "\r\n",
        "  def reset_states(self):\r\n",
        "    return self.auc.reset_states()\r\n",
        "\r\n",
        "  def get_config(self):\r\n",
        "    return self.auc.get_config()\r\n",
        "\r\n",
        "  @property\r\n",
        "  def thresholds(self):\r\n",
        "    return self.auc.thresholds"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvlTpfOyxEHa"
      },
      "source": [
        "# Just subclassing Loss, doesnt give us the power how to weight the sample losses.\r\n",
        "# Hence re-implementing the __call__ method\r\n",
        "class MaskedBCELoss(keras.losses.Loss):\r\n",
        "  def __init__(self, **kwargs):\r\n",
        "    super().__init__(**kwargs)\r\n",
        "\r\n",
        "    self.bce = keras.losses.BinaryCrossentropy(reduction=keras.losses.Reduction.NONE)\r\n",
        "\r\n",
        "  def call(self, y_true, y_pred, sample_weight):\r\n",
        "\r\n",
        "    normal_bce_loss = self.bce(y_true, y_pred, sample_weight) # gives 0 where masked.\r\n",
        "\r\n",
        "    # count # of non masked entries in the batch\r\n",
        "    unmasked_count = tf.math.reduce_sum(tf.cast(tf.math.not_equal(normal_bce_loss, 0), tf.float32))\r\n",
        "\r\n",
        "    # sum the unmasked entries.\r\n",
        "    unmasked_sum = tf.math.reduce_sum(normal_bce_loss)\r\n",
        "  \r\n",
        "    average_loss = tf.math.divide(unmasked_sum, unmasked_count)\r\n",
        "\r\n",
        "    return average_loss\r\n",
        "\r\n",
        "  def __call__(self, y_true, y_pred, sample_weight):\r\n",
        "\r\n",
        "    graph_ctx = tf_utils.graph_context_for_symbolic_tensors(\r\n",
        "        y_true, y_pred, sample_weight)\r\n",
        "    with K.name_scope(self._name_scope), graph_ctx:\r\n",
        "      ag_call = autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\r\n",
        "      losses = ag_call(y_true, y_pred, sample_weight)\r\n",
        "      return losses\r\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Q_LtMP58mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd38a0b-6dc5-440e-e692-45214f0850eb"
      },
      "source": [
        "loss = MaskedBCELoss()\r\n",
        "loss(yb, y_pred, mb)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.76593995>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go7VdsP2132B"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
        "  def __init__(self, d_model, warmup_steps=4000):\r\n",
        "    super(CustomSchedule, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\r\n",
        "\r\n",
        "    self.warmup_steps = warmup_steps\r\n",
        "\r\n",
        "  def __call__(self, step):\r\n",
        "    arg1 = tf.math.rsqrt(step)\r\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\r\n",
        "\r\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOhUhp9OmsjJ"
      },
      "source": [
        "with strategy.scope():\n",
        "  learning_rate = CustomSchedule(experiment_config[\"model_args\"][\"d_model\"])\n",
        "  optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "  # loss = keras.losses.BinaryCrossentropy()\n",
        "  loss = MaskedBCELoss()\n",
        "  reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-7, verbose=1) # cannot be used with custom scheduler\n",
        "  model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"model/best-model.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "  wandb_cb = [WandbCallback(monitor=\"val_loss\", mode=\"min\", save_weights_only=True, verbose=1)]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCXEmmaUVIVh"
      },
      "source": [
        "# update the wandb config\r\n",
        "wandb.config.update(experiment_config)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo7o34xVoeFc"
      },
      "source": [
        "with strategy.scope():\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=[keras.metrics.AUC()]) #, weighted_metrics=[CustomAUC()]) # \"weighted_metrics\" not supported on TPU with tf.data"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdpViganz2pH"
      },
      "source": [
        "## OVERFIT SINGLE BATCH\n",
        "# with strategy.scope():\n",
        "#   x, y, mask = next(iter(dataset.take(1))) # cannot just use take(), cause that return different batch everytime\n",
        "#   print(x.shape, y.shape, mask.shape)\n",
        "#   model.fit(x, y, epochs=100, sample_weight=mask)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEAl6-FkbGOW",
        "outputId": "2c3b06fd-1bcc-4fc2-8cd9-de568ac44628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir model"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Lfz06ApMcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd24517-50c5-4cdc-d639-187274ac2093"
      },
      "source": [
        "with strategy.scope():\n",
        "  history = model.fit(dataset, validation_data=valid_dataset, epochs=experiment_config[\"training_args\"][\"epochs\"], callbacks=[model_checkpoint_cb, wandb_cb])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1345/1345 [==============================] - 410s 274ms/step - loss: 0.6634 - auc: 0.1862 - val_loss: 0.5969 - val_auc: 0.5261\n",
            "Epoch 00000: val_loss improved from inf to 0.59686, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 2/30\n",
            "1345/1345 [==============================] - 366s 270ms/step - loss: 0.5892 - auc: 0.5089 - val_loss: 0.5669 - val_auc: 0.6014\n",
            "Epoch 00001: val_loss improved from 0.59686 to 0.56692, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 3/30\n",
            "1345/1345 [==============================] - 366s 270ms/step - loss: 0.5665 - auc: 0.5899 - val_loss: 0.5600 - val_auc: 0.6663\n",
            "Epoch 00002: val_loss improved from 0.56692 to 0.56001, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 4/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5594 - auc: 0.5855 - val_loss: 0.5544 - val_auc: 0.6717\n",
            "Epoch 00003: val_loss improved from 0.56001 to 0.55444, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 5/30\n",
            "1345/1345 [==============================] - 366s 271ms/step - loss: 0.5546 - auc: 0.6274 - val_loss: 0.5545 - val_auc: 0.6173\n",
            "Epoch 6/30\n",
            "1345/1345 [==============================] - 366s 271ms/step - loss: 0.5523 - auc: 0.6167 - val_loss: 0.5512 - val_auc: 0.5897\n",
            "Epoch 00005: val_loss improved from 0.55444 to 0.55121, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 7/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5506 - auc: 0.6004 - val_loss: 0.5514 - val_auc: 0.5752\n",
            "Epoch 8/30\n",
            "1345/1345 [==============================] - 366s 270ms/step - loss: 0.5492 - auc: 0.6082 - val_loss: 0.5494 - val_auc: 0.6459\n",
            "Epoch 00007: val_loss improved from 0.55121 to 0.54943, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 9/30\n",
            "1345/1345 [==============================] - 366s 271ms/step - loss: 0.5478 - auc: 0.6440 - val_loss: 0.5487 - val_auc: 0.6134\n",
            "Epoch 00008: val_loss improved from 0.54943 to 0.54872, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 10/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5464 - auc: 0.6567 - val_loss: 0.5479 - val_auc: 0.7395\n",
            "Epoch 00009: val_loss improved from 0.54872 to 0.54793, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 11/30\n",
            "1345/1345 [==============================] - 366s 270ms/step - loss: 0.5452 - auc: 0.7054 - val_loss: 0.5473 - val_auc: 0.6699\n",
            "Epoch 00010: val_loss improved from 0.54793 to 0.54732, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 12/30\n",
            "1345/1345 [==============================] - 366s 271ms/step - loss: 0.5441 - auc: 0.6809 - val_loss: 0.5460 - val_auc: 0.7001\n",
            "Epoch 00011: val_loss improved from 0.54732 to 0.54597, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 13/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5431 - auc: 0.6752 - val_loss: 0.5468 - val_auc: 0.7017\n",
            "Epoch 14/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5422 - auc: 0.6621 - val_loss: 0.5467 - val_auc: 0.6370\n",
            "Epoch 15/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5413 - auc: 0.6741 - val_loss: 0.5447 - val_auc: 0.6714\n",
            "Epoch 00014: val_loss improved from 0.54597 to 0.54469, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 16/30\n",
            "1345/1345 [==============================] - 366s 270ms/step - loss: 0.5405 - auc: 0.7033 - val_loss: 0.5435 - val_auc: 0.6949\n",
            "Epoch 00015: val_loss improved from 0.54469 to 0.54345, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 17/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5398 - auc: 0.7048 - val_loss: 0.5435 - val_auc: 0.7268\n",
            "Epoch 18/30\n",
            "1345/1345 [==============================] - 366s 271ms/step - loss: 0.5390 - auc: 0.7270 - val_loss: 0.5436 - val_auc: 0.7440\n",
            "Epoch 19/30\n",
            "1345/1345 [==============================] - 368s 272ms/step - loss: 0.5386 - auc: 0.7277 - val_loss: 0.5433 - val_auc: 0.7031\n",
            "Epoch 00018: val_loss improved from 0.54345 to 0.54329, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 20/30\n",
            "1345/1345 [==============================] - 368s 272ms/step - loss: 0.5380 - auc: 0.7333 - val_loss: 0.5436 - val_auc: 0.6966\n",
            "Epoch 21/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5372 - auc: 0.7321 - val_loss: 0.5443 - val_auc: 0.7492\n",
            "Epoch 22/30\n",
            "1345/1345 [==============================] - 366s 270ms/step - loss: 0.5368 - auc: 0.7529 - val_loss: 0.5426 - val_auc: 0.7586\n",
            "Epoch 00021: val_loss improved from 0.54329 to 0.54263, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 23/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5363 - auc: 0.7427 - val_loss: 0.5428 - val_auc: 0.7674\n",
            "Epoch 24/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5358 - auc: 0.7506 - val_loss: 0.5429 - val_auc: 0.7368\n",
            "Epoch 25/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5357 - auc: 0.7545 - val_loss: 0.5422 - val_auc: 0.7728\n",
            "Epoch 00024: val_loss improved from 0.54263 to 0.54224, saving model to /content/wandb/run-20201230_055544-3sz12zm2/files/model-best.h5\n",
            "Epoch 26/30\n",
            "1345/1345 [==============================] - 368s 272ms/step - loss: 0.5351 - auc: 0.7933 - val_loss: 0.5426 - val_auc: 0.7739\n",
            "Epoch 27/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5348 - auc: 0.8016 - val_loss: 0.5427 - val_auc: 0.8009\n",
            "Epoch 28/30\n",
            "1345/1345 [==============================] - 368s 272ms/step - loss: 0.5345 - auc: 0.8092 - val_loss: 0.5439 - val_auc: 0.7731\n",
            "Epoch 29/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5340 - auc: 0.7944 - val_loss: 0.5435 - val_auc: 0.7740\n",
            "Epoch 30/30\n",
            "1345/1345 [==============================] - 367s 271ms/step - loss: 0.5334 - auc: 0.7959 - val_loss: 0.5427 - val_auc: 0.7721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKZXGWf_t8gt"
      },
      "source": [
        "#### Validation - Latest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2YSjqrXt4lL",
        "outputId": "a0d28c2a-2720-48c4-96fd-1da5b2789d17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\r\n",
        "with strategy.scope():\r\n",
        "  metric = keras.metrics.AUC()\r\n",
        "\r\n",
        "  for valid_xb, valid_yb, valid_mb in valid_dataset:\r\n",
        "    valid_y_pred = model(valid_xb, training=False)\r\n",
        "    metric.update_state(valid_yb, valid_y_pred, valid_mb)\r\n",
        "\r\n",
        "metric.result().numpy()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7758853"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V03flqmlb0ya"
      },
      "source": [
        "#### Validation - Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrWT0dtGb15Z",
        "outputId": "3938504f-6413-45ba-a48e-669a41b57d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with strategy.scope():\r\n",
        "  new_model = TransformerSeq2SeqClassifier(\r\n",
        "        num_layers=experiment_config[\"model_args\"][\"num_layers\"],\r\n",
        "        d_model=experiment_config[\"model_args\"][\"d_model\"],\r\n",
        "        num_heads=experiment_config[\"model_args\"][\"num_heads\"],\r\n",
        "        dff=experiment_config[\"model_args\"][\"dff\"],\r\n",
        "        maximum_position_encoding=SEQ_LEN,\r\n",
        "        embed_size_dict=embed_sizes\r\n",
        "    )\r\n",
        "  new_model.build(input_shape=(128, SEQ_LEN+1, 197)) # input_shape - [batch_size, seq_len, features]\r\n",
        "  new_model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer_seq2seq_classifier_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_1 (Encoder)          multiple                  14773248  \n",
            "_________________________________________________________________\n",
            "decoder_1 (Decoder)          multiple                  15304192  \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             multiple                  513       \n",
            "=================================================================\n",
            "Total params: 30,077,953\n",
            "Trainable params: 30,077,953\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhiK_llFb5Nu"
      },
      "source": [
        "new_model.load_weights(\"model/best-model.h5\")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLHwaHDub91B",
        "outputId": "5e100860-f265-4ea0-c149-6cd6b1d1c32e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\r\n",
        "with strategy.scope():\r\n",
        "  metric = keras.metrics.AUC()\r\n",
        "\r\n",
        "  for valid_xb, valid_yb, valid_mb in valid_dataset:\r\n",
        "    valid_y_pred = new_model(valid_xb, training=False)\r\n",
        "    metric.update_state(valid_yb, valid_y_pred, valid_mb)\r\n",
        "\r\n",
        "auc = metric.result().numpy()\r\n",
        "auc"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.77644473"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB_XBPoaAEEe"
      },
      "source": [
        "wandb.log({\"best_model_auc\": auc})"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykJxj_Muynn3",
        "outputId": "141cd0e7-e12b-4128-8857-d7f02d94bed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684,
          "referenced_widgets": [
            "5c0cfe4ac88b4f7eb9bbe140fcc5524b",
            "08ac3215becd491a97f2bbe4f9f8f54b",
            "d98106bb7ab44a27a922f2a250d999f5",
            "3a1fc395589048ba8c451ab966d0b848",
            "8d8dc44d56134088a171f951f569277c",
            "4f2f248b8aec4f009adf9133e532250f",
            "45e6e05f363343a9bf4f4336f086f744",
            "b66a376faef24a6280ab43ffb3088a25"
          ]
        }
      },
      "source": [
        "wandb_run.finish()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 855<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c0cfe4ac88b4f7eb9bbe140fcc5524b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 114.81MB of 114.81MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20201230_055544-3sz12zm2/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20201230_055544-3sz12zm2/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.53348</td></tr><tr><td>auc</td><td>0.79271</td></tr><tr><td>val_loss</td><td>0.54269</td></tr><tr><td>val_auc</td><td>0.77212</td></tr><tr><td>_step</td><td>30</td></tr><tr><td>_runtime</td><td>16334</td></tr><tr><td>_timestamp</td><td>1609324078</td></tr><tr><td>best_val_loss</td><td>0.54224</td></tr><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_model_auc</td><td>0.77644</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>auc</td><td>▁▅▅▅▆▆▅▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▃▅▅▃▃▂▄▃▆▅▅▅▄▅▅▆▇▆▅▇▇▇▆▇▇█▇▇▇</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆█</td></tr><tr><td>best_model_auc</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">autumn-gorge-27</strong>: <a href=\"https://wandb.ai/nisarahamedk/kaggle-riid/runs/3sz12zm2\" target=\"_blank\">https://wandb.ai/nisarahamedk/kaggle-riid/runs/3sz12zm2</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS2RWxD60rYm"
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRE5KRsYboLs"
      },
      "source": [
        "#### Upload to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g4mBjLKpj2l"
      },
      "source": [
        "%%capture\r\n",
        "!pip install kaggle"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TzMag2xp6yh"
      },
      "source": [
        "if experiment_config[\"env_args\"][\"is_kaggle\"]:\r\n",
        "  from google.colab import drive\r\n",
        "  drive.mount(\"/content/drive\")\r\n",
        "\r\n",
        "  # Copy Kaggle API key\r\n",
        "  !mkdir -p ~/.kaggle && cp /content/drive/My\\ Drive/Projects/Kaggle/api_key/kaggle.json ~/.kaggle/\r\n",
        "\r\n",
        "  !kaggle datasets init -p model/\r\n",
        "\r\n",
        "  # id and title only alphanumeric and \"-\"\r\n",
        "  meta = \"\"\"\r\n",
        "  {\r\n",
        "    \"licenses\": [\r\n",
        "      {\r\n",
        "        \"name\": \"CC0-1.0\"\r\n",
        "      }\r\n",
        "    ], \r\n",
        "    \"id\": \"nisarahamedk/riid-model-2\",\r\n",
        "    \"title\": \"riid-model-2\"\r\n",
        "  }\r\n",
        "  \"\"\"\r\n",
        "  with open(\"model/dataset-metadata.json\", \"w\") as f:\r\n",
        "    f.write(meta)\r\n",
        "\r\n",
        "  # create\r\n",
        "  !kaggle datasets create -p model/ --dir-mode tar -u"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j-xlXAOkej-"
      },
      "source": [
        ""
      ],
      "execution_count": 82,
      "outputs": []
    }
  ]
}