{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.023676,
     "end_time": "2020-12-18T10:46:38.029984",
     "exception": false,
     "start_time": "2020-12-18T10:46:38.006308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nisarahamedk/kaggle-riid/blob/master/notebooks/RIID_TF_Transformer_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIIMFhdG1L1o",
    "papermill": {
     "duration": 0.022642,
     "end_time": "2020-12-18T10:46:38.075185",
     "exception": false,
     "start_time": "2020-12-18T10:46:38.052543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RIID TF Transformer Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:38.126485Z",
     "iopub.status.busy": "2020-12-18T10:46:38.125843Z",
     "iopub.status.idle": "2020-12-18T10:46:43.309207Z",
     "shell.execute_reply": "2020-12-18T10:46:43.309761Z"
    },
    "id": "KyaJuEug1SjH",
    "papermill": {
     "duration": 5.211534,
     "end_time": "2020-12-18T10:46:43.309965",
     "exception": false,
     "start_time": "2020-12-18T10:46:38.098431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.369294Z",
     "iopub.status.busy": "2020-12-18T10:46:43.368328Z",
     "iopub.status.idle": "2020-12-18T10:46:43.372340Z",
     "shell.execute_reply": "2020-12-18T10:46:43.371590Z"
    },
    "id": "djh3Uu2t5gW0",
    "papermill": {
     "duration": 0.037419,
     "end_time": "2020-12-18T10:46:43.372591",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.335172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqfXTHol5tsO",
    "papermill": {
     "duration": 0.025844,
     "end_time": "2020-12-18T10:46:43.423870",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.398026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7Etv2NO5zT2",
    "papermill": {
     "duration": 0.023053,
     "end_time": "2020-12-18T10:46:43.472811",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.449758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.520969Z",
     "iopub.status.busy": "2020-12-18T10:46:43.520339Z",
     "iopub.status.idle": "2020-12-18T10:46:43.524792Z",
     "shell.execute_reply": "2020-12-18T10:46:43.525240Z"
    },
    "id": "n6vaouEE53Ht",
    "papermill": {
     "duration": 0.029522,
     "end_time": "2020-12-18T10:46:43.525351",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.495829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.572092Z",
     "iopub.status.busy": "2020-12-18T10:46:43.571521Z",
     "iopub.status.idle": "2020-12-18T10:46:43.578095Z",
     "shell.execute_reply": "2020-12-18T10:46:43.578506Z"
    },
    "id": "xxFYxFOo55M1",
    "papermill": {
     "duration": 0.031062,
     "end_time": "2020-12-18T10:46:43.578642",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.547580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZbzX60E58BU",
    "papermill": {
     "duration": 0.021699,
     "end_time": "2020-12-18T10:46:43.623187",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.601488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Look ahead maskÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.671081Z",
     "iopub.status.busy": "2020-12-18T10:46:43.670493Z",
     "iopub.status.idle": "2020-12-18T10:46:43.674785Z",
     "shell.execute_reply": "2020-12-18T10:46:43.675298Z"
    },
    "id": "zPGr-jHm5---",
    "papermill": {
     "duration": 0.029745,
     "end_time": "2020-12-18T10:46:43.675422",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.645677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEG9-yfd6B4z",
    "papermill": {
     "duration": 0.02223,
     "end_time": "2020-12-18T10:46:43.720435",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.698205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.768929Z",
     "iopub.status.busy": "2020-12-18T10:46:43.768339Z",
     "iopub.status.idle": "2020-12-18T10:46:43.777869Z",
     "shell.execute_reply": "2020-12-18T10:46:43.777360Z"
    },
    "id": "YJQ-2NrS6FNc",
    "papermill": {
     "duration": 0.034817,
     "end_time": "2020-12-18T10:46:43.777976",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.743159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRn4GRyP6H19",
    "papermill": {
     "duration": 0.022884,
     "end_time": "2020-12-18T10:46:43.823550",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.800666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.887273Z",
     "iopub.status.busy": "2020-12-18T10:46:43.886223Z",
     "iopub.status.idle": "2020-12-18T10:46:43.889196Z",
     "shell.execute_reply": "2020-12-18T10:46:43.888530Z"
    },
    "id": "gxrdILh26KPP",
    "papermill": {
     "duration": 0.042161,
     "end_time": "2020-12-18T10:46:43.889312",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.847151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8iz9DQL61_Q",
    "papermill": {
     "duration": 0.02322,
     "end_time": "2020-12-18T10:46:43.935762",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.912542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Pointwise FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:43.987129Z",
     "iopub.status.busy": "2020-12-18T10:46:43.986420Z",
     "iopub.status.idle": "2020-12-18T10:46:43.989672Z",
     "shell.execute_reply": "2020-12-18T10:46:43.989023Z"
    },
    "id": "jP7TSLvH65Ag",
    "papermill": {
     "duration": 0.031256,
     "end_time": "2020-12-18T10:46:43.989777",
     "exception": false,
     "start_time": "2020-12-18T10:46:43.958521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeudaHGQ67H6",
    "papermill": {
     "duration": 0.023549,
     "end_time": "2020-12-18T10:46:44.036966",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.013417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:44.094479Z",
     "iopub.status.busy": "2020-12-18T10:46:44.093719Z",
     "iopub.status.idle": "2020-12-18T10:46:44.096621Z",
     "shell.execute_reply": "2020-12-18T10:46:44.097086Z"
    },
    "id": "5bcO_5Y-6-oi",
    "papermill": {
     "duration": 0.037404,
     "end_time": "2020-12-18T10:46:44.097209",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.059805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbPkooHe7BRD",
    "papermill": {
     "duration": 0.023238,
     "end_time": "2020-12-18T10:46:44.143484",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.120246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:44.207548Z",
     "iopub.status.busy": "2020-12-18T10:46:44.206839Z",
     "iopub.status.idle": "2020-12-18T10:46:44.210232Z",
     "shell.execute_reply": "2020-12-18T10:46:44.209702Z"
    },
    "id": "cGUaKNGc7Dy8",
    "papermill": {
     "duration": 0.04359,
     "end_time": "2020-12-18T10:46:44.210329",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.166739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.content_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"content_id\"] + 1, d_model)\n",
    "    self.task_container_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"task_container_id\"] + 1, d_model)\n",
    "    self.part_emb = tf.keras.layers.Embedding(embed_size_dict[\"part\"] + 2, d_model)\n",
    "    self.prior_question_elapsed_time_emb = tf.keras.layers.Dense(d_model, use_bias=True)\n",
    "    self.prev_answered_emb = tf.keras.layers.Embedding(4, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "\n",
    "\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "\n",
    "    # adding embeddings and position encoding.\n",
    "    c_emb = self.content_id_emb(x[..., 0])  # (batch_size, input_seq_len, d_model)\n",
    "    t_emb = self.task_container_id_emb(x[..., 1])\n",
    "    prior_time_emb = self.prior_question_elapsed_time_emb(tf.expand_dims(x[..., 2], axis=-1))\n",
    "    pt_emb = self.part_emb(x[..., 3])\n",
    "    pv_emb = self.prev_answered_emb(x[..., 4])\n",
    "    x = c_emb + t_emb + prior_time_emb + pt_emb + pv_emb\n",
    "    \n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask) # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:44.277203Z",
     "iopub.status.busy": "2020-12-18T10:46:44.276499Z",
     "iopub.status.idle": "2020-12-18T10:46:44.279305Z",
     "shell.execute_reply": "2020-12-18T10:46:44.278825Z"
    },
    "id": "vanS2PyP7Hgo",
    "papermill": {
     "duration": 0.037334,
     "end_time": "2020-12-18T10:46:44.279418",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.242084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerSeq2SeqClassifier(keras.models.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate=0.1):\n",
    "    super(TransformerSeq2SeqClassifier, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate)\n",
    "    self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "  def call(self, x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "    encoded = self.encoder(x, mask=look_ahead_mask)\n",
    "\n",
    "    out = self.out(encoded)\n",
    "    return out # [batch_size, input_seq_len, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v2XHNJ77Jet",
    "papermill": {
     "duration": 0.023233,
     "end_time": "2020-12-18T10:46:44.326230",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.302997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:44.377288Z",
     "iopub.status.busy": "2020-12-18T10:46:44.376744Z",
     "iopub.status.idle": "2020-12-18T10:46:44.379287Z",
     "shell.execute_reply": "2020-12-18T10:46:44.378807Z"
    },
    "id": "0kDeBRYH7Q_8",
    "papermill": {
     "duration": 0.030135,
     "end_time": "2020-12-18T10:46:44.379402",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.349267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = 'gs://kds-f48a9c4d95386273c0ef508e337abd3f874b82a454a6c3d0e035839c'\n",
    "DATA_PATH = \"/kaggle/input/riid-0-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:44.467027Z",
     "iopub.status.busy": "2020-12-18T10:46:44.466323Z",
     "iopub.status.idle": "2020-12-18T10:46:44.488302Z",
     "shell.execute_reply": "2020-12-18T10:46:44.488830Z"
    },
    "id": "_poqM13u7RQd",
    "outputId": "6f50b5c6-560c-4d34-ad86-ec247470a1ed",
    "papermill": {
     "duration": 0.086218,
     "end_time": "2020-12-18T10:46:44.488984",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.402766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_id': 32736, 'task_container_id': 9999, 'part': 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sizes = pickle.loads(tf.io.read_file(DATA_PATH + \"/emb_sz.pkl\").numpy())\n",
    "embed_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:44.542336Z",
     "iopub.status.busy": "2020-12-18T10:46:44.541447Z",
     "iopub.status.idle": "2020-12-18T10:46:45.592434Z",
     "shell.execute_reply": "2020-12-18T10:46:45.592902Z"
    },
    "id": "Q-3uU1WM7YDF",
    "outputId": "59c0c7cb-6397-49fb-c849-f5bf98333bfd",
    "papermill": {
     "duration": 1.078639,
     "end_time": "2020-12-18T10:46:45.593039",
     "exception": false,
     "start_time": "2020-12-18T10:46:44.514400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_seq2seq_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  23991808  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  513       \n",
      "=================================================================\n",
      "Total params: 23,992,321\n",
      "Trainable params: 23,992,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TransformerSeq2SeqClassifier(\n",
    "      num_layers=1,\n",
    "      d_model=512,\n",
    "      num_heads=8,\n",
    "      dff=1024,\n",
    "      maximum_position_encoding=128,\n",
    "      embed_size_dict=embed_sizes\n",
    "  )\n",
    "model.build(input_shape=(128, 128, 5)) # input_shape - [batch_size, seq_len, features]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:45.661178Z",
     "iopub.status.busy": "2020-12-18T10:46:45.660571Z",
     "iopub.status.idle": "2020-12-18T10:46:48.090095Z",
     "shell.execute_reply": "2020-12-18T10:46:48.090633Z"
    },
    "id": "za2z4w357kEQ",
    "papermill": {
     "duration": 2.472875,
     "end_time": "2020-12-18T10:46:48.090832",
     "exception": false,
     "start_time": "2020-12-18T10:46:45.617957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/kaggle/input/riid-model-0/best-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Hmlvi870T4",
    "papermill": {
     "duration": 0.024869,
     "end_time": "2020-12-18T10:46:48.140564",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.115695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuunfOds71fi",
    "papermill": {
     "duration": 0.024675,
     "end_time": "2020-12-18T10:46:48.189750",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.165075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Question df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.243508Z",
     "iopub.status.busy": "2020-12-18T10:46:48.242979Z",
     "iopub.status.idle": "2020-12-18T10:46:48.271040Z",
     "shell.execute_reply": "2020-12-18T10:46:48.271532Z"
    },
    "papermill": {
     "duration": 0.056284,
     "end_time": "2020-12-18T10:46:48.271671",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.215387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\"/kaggle/input/riiid-test-answer-prediction/questions.csv\", usecols=[0,3], index_col=\"question_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025043,
     "end_time": "2020-12-18T10:46:48.321804",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.296761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.379311Z",
     "iopub.status.busy": "2020-12-18T10:46:48.378771Z",
     "iopub.status.idle": "2020-12-18T10:46:48.404604Z",
     "shell.execute_reply": "2020-12-18T10:46:48.404051Z"
    },
    "papermill": {
     "duration": 0.057206,
     "end_time": "2020-12-18T10:46:48.404777",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.347571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = riiideducation.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.459056Z",
     "iopub.status.busy": "2020-12-18T10:46:48.458400Z",
     "iopub.status.idle": "2020-12-18T10:46:48.461923Z",
     "shell.execute_reply": "2020-12-18T10:46:48.462501Z"
    },
    "papermill": {
     "duration": 0.032684,
     "end_time": "2020-12-18T10:46:48.462645",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.429961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can only iterate through a result from `env.iter_test()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.519521Z",
     "iopub.status.busy": "2020-12-18T10:46:48.518830Z",
     "iopub.status.idle": "2020-12-18T10:46:48.524593Z",
     "shell.execute_reply": "2020-12-18T10:46:48.525195Z"
    },
    "papermill": {
     "duration": 0.035823,
     "end_time": "2020-12-18T10:46:48.525356",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.489533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes_train = {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'boolean'\n",
    "    }\n",
    "\n",
    "dtypes_questions = {\n",
    "    \"question_id\": \"\",\n",
    "    \"bundle_id\": \"\",\n",
    "    \"correct_answer\": \"\",\n",
    "    \"part\": \"int16\",\n",
    "    \"tags\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.582483Z",
     "iopub.status.busy": "2020-12-18T10:46:48.581815Z",
     "iopub.status.idle": "2020-12-18T10:46:48.586382Z",
     "shell.execute_reply": "2020-12-18T10:46:48.585737Z"
    },
    "papermill": {
     "duration": 0.034114,
     "end_time": "2020-12-18T10:46:48.586526",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.552412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.653183Z",
     "iopub.status.busy": "2020-12-18T10:46:48.652440Z",
     "iopub.status.idle": "2020-12-18T10:46:48.654757Z",
     "shell.execute_reply": "2020-12-18T10:46:48.655299Z"
    },
    "papermill": {
     "duration": 0.042892,
     "end_time": "2020-12-18T10:46:48.655420",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.612528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def pad(a, seq_len, max_seq_len):\n",
    "  s = max_seq_len - seq_len\n",
    "  # making [[0, 0], [s, 0]]\n",
    "  r = tf.stack([s, tf.constant(0)])\n",
    "  t = tf.stack([tf.constant([0, 0]), r])\n",
    "  \n",
    "  return tf.pad(a, t) # ,1 to debug\n",
    "\n",
    "@tf.function\n",
    "def trim(a, seq_len,  max_seq_len):\n",
    "  start = tf.squeeze(tf.random.uniform((1,), maxval=(seq_len-max_seq_len), dtype=tf.int32))\n",
    "  # https://www.quora.com/How-does-tf-slice-work-in-TensorFlow\n",
    "  begin = tf.stack([tf.constant(0), start])\n",
    "  size = tf.stack([tf.shape(a)[0], max_seq_len])\n",
    "  \n",
    "  return tf.slice(a, begin, size) # , start - to debug\n",
    "\n",
    "@tf.function\n",
    "def pad_or_trim(a):\n",
    "  seq_len = tf.shape(a)[-1]\n",
    "  max_seq_len = SEQ_LEN\n",
    "  fn = tf.cond(tf.less_equal(seq_len, max_seq_len), lambda: pad(a, seq_len, max_seq_len), lambda: trim(a, seq_len, max_seq_len))\n",
    "  return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.714923Z",
     "iopub.status.busy": "2020-12-18T10:46:48.714234Z",
     "iopub.status.idle": "2020-12-18T10:46:48.717067Z",
     "shell.execute_reply": "2020-12-18T10:46:48.717470Z"
    },
    "papermill": {
     "duration": 0.035487,
     "end_time": "2020-12-18T10:46:48.717598",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.682111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def split_x_mask(x):\n",
    "  x = tf.transpose(x, (1, 0)) # [seq_len, n_features]\n",
    "  pad_mask = tf.cast(tf.math.reduce_any(tf.math.not_equal(x, 0), axis=-1), dtype=tf.float32)\n",
    "  return x, tf.expand_dims(pad_mask, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025608,
     "end_time": "2020-12-18T10:46:48.769093",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.743485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:48.833778Z",
     "iopub.status.busy": "2020-12-18T10:46:48.828389Z",
     "iopub.status.idle": "2020-12-18T10:46:51.152895Z",
     "shell.execute_reply": "2020-12-18T10:46:51.153848Z"
    },
    "papermill": {
     "duration": 2.359614,
     "end_time": "2020-12-18T10:46:51.154010",
     "exception": false,
     "start_time": "2020-12-18T10:46:48.794396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Total time: 2.31 sec\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "for test_dfm, sample_pred in iter_test:\n",
    "    \n",
    "    # filtering only questions, removing lectures.\n",
    "    test_dfm = test_dfm[test_dfm.content_type_id == False]\n",
    "    \n",
    "    # selecting required cols\n",
    "    test_df = test_dfm[[\"user_id\",\"content_id\",\"task_container_id\",\"prior_question_elapsed_time\"]]\n",
    "    \n",
    "    # join question for feaures\n",
    "    test_df = test_df.join(questions_df, on=\"content_id\")\n",
    "    \n",
    "    # 0 used for padding so increment all indicator cols\n",
    "    indicator_cols = [\"content_id\", \"task_container_id\", \"part\"]\n",
    "    for c in indicator_cols:\n",
    "        test_df[c] = test_df[c] + 1\n",
    "        \n",
    "    # fillna and convert milliseconds to minutes.\n",
    "    test_df['prior_question_elapsed_time'] = test_df[\"prior_question_elapsed_time\"].fillna(0).astype(np.float32) / 60000\n",
    "    \n",
    "    # FIXME: unseen ids - content_id > 32736, 'task_container_id'> 9999, 'part'> 7\n",
    "    test_df.loc[test_df[\"content_id\"] > 32736, \"content_id\"] = 0\n",
    "    test_df.loc[test_df[\"task_container_id\"] > 9999, \"task_container_id\"] = 0\n",
    "    test_df.loc[test_df[\"part\"] > 7, \"part\"] = 0\n",
    "    test_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # creating user group array\n",
    "    user_groups = test_df.groupby(\"user_id\").apply(\n",
    "        lambda rows: (\n",
    "            rows[\"content_id\"].values.astype(dtypes_train[\"content_id\"]), \n",
    "            rows[\"task_container_id\"].values.astype(dtypes_train[\"task_container_id\"]), \n",
    "            rows[\"prior_question_elapsed_time\"].values.astype(dtypes_train[\"prior_question_elapsed_time\"]),\n",
    "            rows[\"part\"].values.astype(dtypes_questions[\"part\"]),\n",
    "            np.full_like(rows[\"part\"], 3, dtype=dtypes_train[\"answered_correctly\"]) # dummy prev_answered\n",
    "            # (rows[\"answered_correctly\"].shift(fill_value=2)+1).values.astype(dtypes_train[\"answered_correctly\"]), # previous question answered as a feature, 2 as fill (because first q would not have this val), +1 since we consider 0 as padding.\n",
    "            # rows[\"answered_correctly\"].values.astype(dtypes_train[\"answered_correctly\"]),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def gen():\n",
    "        for ug in user_groups:\n",
    "            yield ug\n",
    "            \n",
    "    # tf Dataset\n",
    "    test_ds = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\n",
    "    test_ds = test_ds.map(pad_or_trim, num_parallel_calls=AUTOTUNE) \n",
    "    test_ds = test_ds.map(split_x_mask, num_parallel_calls=AUTOTUNE) # x and mask\n",
    "    test_ds = test_ds.batch(len(user_groups))\n",
    "    \n",
    "    # predict\n",
    "    x, m = next(iter(test_ds))\n",
    "    preds = model.predict(x)\n",
    "    final_preds = tf.boolean_mask(preds, m)\n",
    "    \n",
    "    \n",
    "    # submit\n",
    "    test_dfm[\"answered_correctly\"] = final_preds\n",
    "    env.predict(test_dfm[[\"row_id\", \"answered_correctly\"]])\n",
    "    \n",
    "total_time = datetime.now() - start\n",
    "print(f\"===== Total time: {total_time.total_seconds():.3} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026052,
     "end_time": "2020-12-18T10:46:51.207186",
     "exception": false,
     "start_time": "2020-12-18T10:46:51.181134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T10:46:51.267141Z",
     "iopub.status.busy": "2020-12-18T10:46:51.266526Z",
     "iopub.status.idle": "2020-12-18T10:46:51.269162Z",
     "shell.execute_reply": "2020-12-18T10:46:51.270239Z"
    },
    "papermill": {
     "duration": 0.036559,
     "end_time": "2020-12-18T10:46:51.270470",
     "exception": false,
     "start_time": "2020-12-18T10:46:51.233911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def time(func):\n",
    "#     def wrapped(*args, **kwargs):\n",
    "#         t0 = datetime.now()\n",
    "        \n",
    "#         ret = func(*args, **kwargs)\n",
    "        \n",
    "#         dt = datetime.now() - t0\n",
    "#         print(f\"==> {func.__name__} took {dt.total_seconds():.2} sec\")    \n",
    "#         return ret\n",
    "#     return wrapped\n",
    "\n",
    "# @time\n",
    "# def preprocess(test_dfm):\n",
    "#      # selecting required cols\n",
    "#     test_df = test_dfm[[\"user_id\",\"content_id\",\"task_container_id\",\"prior_question_elapsed_time\"]]\n",
    "    \n",
    "#     # join question for feaures\n",
    "#     test_df = test_df.join(questions_df, on=\"content_id\")\n",
    "    \n",
    "#     # 0 used for padding so increment all indicator cols\n",
    "#     indicator_cols = [\"content_id\", \"task_container_id\", \"part\"]\n",
    "#     for c in indicator_cols:\n",
    "#         test_df[c] = test_df[c] + 1\n",
    "        \n",
    "#     # fillna and convert milliseconds to minutes.\n",
    "#     test_df['prior_question_elapsed_time'] = test_df[\"prior_question_elapsed_time\"].fillna(0).astype(np.float32) / 60000\n",
    "    \n",
    "#     # FIXME: unseen ids - content_id > 32736, 'task_container_id'> 9999, 'part'> 7\n",
    "#     test_df.loc[test_df[\"content_id\"] > 32736, \"content_id\"] = 0\n",
    "#     test_df.loc[test_df[\"task_container_id\"] > 9999, \"task_container_id\"] = 0\n",
    "#     test_df.loc[test_df[\"part\"] > 7, \"part\"] = 0\n",
    "#     test_df.fillna(0, inplace=True)\n",
    "    \n",
    "#     # creating user group array\n",
    "#     user_groups = test_df.groupby(\"user_id\").apply(\n",
    "#         lambda rows: (\n",
    "#             rows[\"content_id\"].values.astype(dtypes_train[\"content_id\"]), \n",
    "#             rows[\"task_container_id\"].values.astype(dtypes_train[\"task_container_id\"]), \n",
    "#             rows[\"prior_question_elapsed_time\"].values.astype(dtypes_train[\"prior_question_elapsed_time\"]),\n",
    "#             rows[\"part\"].values.astype(dtypes_questions[\"part\"]),\n",
    "#             np.full_like(rows[\"part\"], 3, dtype=dtypes_train[\"answered_correctly\"]) # dummy prev_answered\n",
    "#             # (rows[\"answered_correctly\"].shift(fill_value=2)+1).values.astype(dtypes_train[\"answered_correctly\"]), # previous question answered as a feature, 2 as fill (because first q would not have this val), +1 since we consider 0 as padding.\n",
    "#             # rows[\"answered_correctly\"].values.astype(dtypes_train[\"answered_correctly\"]),\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     return user_groups\n",
    "\n",
    "# @time\n",
    "# def create_ds(user_groups):\n",
    "#     def gen():\n",
    "#         for ug in user_groups:\n",
    "#             yield ug\n",
    "            \n",
    "#     # tf Dataset\n",
    "#     test_ds = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\n",
    "#     test_ds = test_ds.map(pad_or_trim, num_parallel_calls=AUTOTUNE) \n",
    "#     test_ds = test_ds.map(split_x_mask, num_parallel_calls=AUTOTUNE) # x and mask\n",
    "#     test_ds = test_ds.batch(len(user_groups))\n",
    "#     return test_ds\n",
    "\n",
    "# @time\n",
    "# def predict(test_ds):\n",
    "    \n",
    "#     # predict\n",
    "#     x, m = next(iter(test_ds))\n",
    "#     preds = model.predict(x)\n",
    "#     final_preds = tf.boolean_mask(preds, m)\n",
    "    \n",
    "#     return final_preds\n",
    "\n",
    "# start = datetime.now()\n",
    "\n",
    "# for test_dfm, sample_pred in iter_test:\n",
    "#     print(\"--- iteration ---\")\n",
    "#     print(len(test_dfm))\n",
    "#     # filtering only questions, removing lectures.\n",
    "#     test_dfm = test_dfm[test_dfm.content_type_id == False]\n",
    "#     print(len(test_dfm))\n",
    "    \n",
    "#     user_groups = preprocess(test_dfm)\n",
    "    \n",
    "#     test_ds = create_ds(user_groups)    \n",
    "    \n",
    "#     final_preds = predict(test_ds)\n",
    "    \n",
    "    \n",
    "#     # submit\n",
    "#     test_dfm[\"answered_correctly\"] = final_preds\n",
    "#     env.predict(test_dfm[[\"row_id\", \"answered_correctly\"]])\n",
    "    \n",
    "# total_time = datetime.now() - start\n",
    "# print(f\"===== Total time: {total_time.total_seconds():.3} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025046,
     "end_time": "2020-12-18T10:46:51.325783",
     "exception": false,
     "start_time": "2020-12-18T10:46:51.300737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026104,
     "end_time": "2020-12-18T10:46:51.378518",
     "exception": false,
     "start_time": "2020-12-18T10:46:51.352414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 18.814096,
   "end_time": "2020-12-18T10:46:52.505600",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-18T10:46:33.691504",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
