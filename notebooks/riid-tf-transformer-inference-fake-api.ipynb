{"cells":[{"metadata":{"id":"view-in-github"},"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/nisarahamedk/kaggle-riid/blob/master/notebooks/RIID_TF_Transformer_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"},{"metadata":{"id":"AIIMFhdG1L1o"},"cell_type":"markdown","source":"### RIID TF Transformer Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!rm -f /opt/conda/lib/python3.7/site-packages/llvmlite-0.31.0-py3.7.egg-info\n!pip install ../input/sparse-package-1/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl\n!pip install ../input/sparse-package-1/setuptools-51.1.1-py3-none-any.whl\n!pip install ../input/sparse-package-1/scipy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/sparse-package-1/llvmlite-0.35.0-cp37-cp37m-manylinux2010_x86_64.whl\n!pip install ../input/sparse-package-1/numba-0.52.0-cp37-cp37m-manylinux2014_x86_64.whl\n!pip install ../input/sparse-package-1/sparse-0.11.2-py2.py3-none-any.whl","execution_count":1,"outputs":[]},{"metadata":{"id":"KyaJuEug1SjH","trusted":true},"cell_type":"code","source":"import itertools\nimport pickle\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport pandas as pd\nimport numpy as np\nimport sparse\n\nnp.random.seed(42)\ntf.random.set_seed(42)","execution_count":2,"outputs":[]},{"metadata":{"id":"djh3Uu2t5gW0","trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"id":"FqfXTHol5tsO"},"cell_type":"markdown","source":"### Model"},{"metadata":{"id":"N7Etv2NO5zT2"},"cell_type":"markdown","source":"#### Positional Encoding"},{"metadata":{"id":"n6vaouEE53Ht","trusted":true},"cell_type":"code","source":"def get_angles(pos, i, d_model):\n  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n  return pos * angle_rates","execution_count":null,"outputs":[]},{"metadata":{"id":"xxFYxFOo55M1","trusted":true},"cell_type":"code","source":"def positional_encoding(position, d_model):\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n\n  # apply sin to even indices in the array; 2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n  # apply cos to odd indices in the array; 2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n  pos_encoding = angle_rads[np.newaxis, ...]\n\n  return tf.cast(pos_encoding, dtype=tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ZbzX60E58BU"},"cell_type":"markdown","source":"#### Look ahead maskÂ¶\n"},{"metadata":{"id":"zPGr-jHm5---","trusted":true},"cell_type":"code","source":"def create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask  # (seq_len, seq_len)","execution_count":null,"outputs":[]},{"metadata":{"id":"wEG9-yfd6B4z"},"cell_type":"markdown","source":"#### Scaled Dot Product Attention"},{"metadata":{"id":"YJQ-2NrS6FNc","trusted":true},"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"id":"hRn4GRyP6H19"},"cell_type":"markdown","source":"#### Multi Head Attention"},{"metadata":{"id":"gxrdILh26KPP","trusted":true},"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n\n    self.dense = tf.keras.layers.Dense(d_model)\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"id":"u8iz9DQL61_Q"},"cell_type":"markdown","source":"#### Pointwise FeedForward Network"},{"metadata":{"id":"jP7TSLvH65Ag","trusted":true},"cell_type":"code","source":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","execution_count":null,"outputs":[]},{"metadata":{"id":"JeudaHGQ67H6"},"cell_type":"markdown","source":"#### EncoderLayer"},{"metadata":{"id":"5bcO_5Y-6-oi","trusted":true},"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n\n  def call(self, x, training, mask):\n\n    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    attn_output = self.dropout1(attn_output, training=training)\n    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n    return out2","execution_count":null,"outputs":[]},{"metadata":{"id":"mbPkooHe7BRD"},"cell_type":"markdown","source":"#### Encoder"},{"metadata":{"id":"cGUaKNGc7Dy8","trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.content_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"content_id\"] + 2, d_model)\n    self.task_container_id_emb = tf.keras.layers.Embedding(embed_size_dict[\"task_container_id\"] + 2, d_model)\n    self.part_emb = tf.keras.layers.Embedding(embed_size_dict[\"part\"] + 2, d_model)\n    self.prior_question_elapsed_time_emb = tf.keras.layers.Dense(d_model, use_bias=True)\n    self.prev_answered_emb = tf.keras.layers.Embedding(4, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                            self.d_model)\n\n\n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n\n    self.dropout = tf.keras.layers.Dropout(rate)\n\n  def call(self, x, training, mask):\n\n    seq_len = tf.shape(x)[1]\n\n    # adding embeddings and position encoding.\n    c_emb = self.content_id_emb(x[..., 0])  # (batch_size, input_seq_len, d_model)\n    t_emb = self.task_container_id_emb(x[..., 1])\n    prior_time_emb = self.prior_question_elapsed_time_emb(tf.expand_dims(x[..., 2], axis=-1))\n    pt_emb = self.part_emb(x[..., 3])\n    pv_emb = self.prev_answered_emb(x[..., 4])\n    x = c_emb + t_emb + prior_time_emb + pt_emb + pv_emb\n    \n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n\n    x = self.dropout(x, training=training)\n\n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x, training, mask) # (batch_size, input_seq_len, d_model)\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{"id":"vanS2PyP7Hgo","trusted":true},"cell_type":"code","source":"class TransformerSeq2SeqClassifier(keras.models.Model):\n  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate=0.1):\n    super(TransformerSeq2SeqClassifier, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, maximum_position_encoding, embed_size_dict, rate)\n    self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n\n  def call(self, x):\n    seq_len = tf.shape(x)[1]\n    look_ahead_mask = create_look_ahead_mask(seq_len)\n    encoded = self.encoder(x, mask=look_ahead_mask)\n\n    out = self.out(encoded)\n    return out # [batch_size, input_seq_len, 1]","execution_count":null,"outputs":[]},{"metadata":{"id":"6v2XHNJ77Jet"},"cell_type":"markdown","source":"#### Embedding sizes"},{"metadata":{"id":"0kDeBRYH7Q_8","trusted":true},"cell_type":"code","source":"# DATA_PATH = 'gs://kds-f48a9c4d95386273c0ef508e337abd3f874b82a454a6c3d0e035839c'\nDATA_PATH = \"/kaggle/input/riid-0-1\"","execution_count":null,"outputs":[]},{"metadata":{"id":"_poqM13u7RQd","outputId":"6f50b5c6-560c-4d34-ad86-ec247470a1ed","trusted":true},"cell_type":"code","source":"embed_sizes = pickle.loads(tf.io.read_file(DATA_PATH + \"/emb_sz.pkl\").numpy())\nembed_sizes","execution_count":null,"outputs":[]},{"metadata":{"id":"Q-3uU1WM7YDF","outputId":"59c0c7cb-6397-49fb-c849-f5bf98333bfd","trusted":true},"cell_type":"code","source":"model = TransformerSeq2SeqClassifier(\n      num_layers=1,\n      d_model=512,\n      num_heads=8,\n      dff=1024,\n      maximum_position_encoding=128,\n      embed_size_dict=embed_sizes\n  )\nmodel.build(input_shape=(128, 128, 5)) # input_shape - [batch_size, seq_len, features]\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"za2z4w357kEQ","trusted":true},"cell_type":"code","source":"model.load_weights(\"/kaggle/input/riid-model-1/best-model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"id":"F9Hmlvi870T4"},"cell_type":"markdown","source":"### Dataset"},{"metadata":{"id":"IuunfOds71fi"},"cell_type":"markdown","source":"#### Question df"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df = pd.read_csv(\"/kaggle/input/riiid-test-answer-prediction/questions.csv\", index_col=\"question_id\")","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test API"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtypes_train = {\n    'row_id': 'int64',\n    'timestamp': 'int64',\n    'user_id': 'int32',\n    'content_id': 'int16',\n    'content_type_id': 'int8',\n    'task_container_id': 'int16',\n    'user_answer': 'int8',\n    'answered_correctly': 'int8',\n    'prior_question_elapsed_time': 'float32',\n    'prior_question_had_explanation': 'boolean'\n    }\n\ndtypes_questions = {\n    \"question_id\": \"\",\n    \"bundle_id\": \"\",\n    \"correct_answer\": \"\",\n    \"part\": \"int16\",\n    \"tags\": \"\",\n}","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 512","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cProfile, pstats, io\nfrom pstats import SortKey","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fake API"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\nimport random\nfrom copy import deepcopy\nimport _pickle as pickle\n\ndef save(file,name, folder = \"\"):\n    if folder != \"\":\n        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n    else:\n        outfile = open(name+'.pickle', 'wb')\n    pickle.dump(file, outfile, protocol=4)\n    outfile.close\n    \ndef load(name, folder = \"\"):\n    if folder != \"\":\n        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n    else:\n        outfile = open(name+'.pickle', 'rb')\n    file = pickle.load(outfile)\n    outfile.close\n    return file","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FakeDataGenerator:\n    \n    def __init__(self):\n        '''\n        self.data will be a dictionnary to iterate over the stored data\n        self.all_rows will be the rows of the train set that are used by the generato\n        self.data_index will be all the data available in the dataset        \n        '''\n        self.data = None\n        self.all_rows = None\n        self.data_index = None\n        return None\n    \n    def __getitem__(self, idx):\n        if idx > self.data_index[-1]:\n            raise StopIteration\n        sample = self.data[idx]\n        sub = sample[['row_id', 'group_num']].copy()\n        sub['answered_correctly'] = np.zeros(sub.shape[0])+0.5\n        return (sample, sub)\n    \n    \n    def load(self, save_name):\n        self.data,self.all_rows = load(save_name)\n        self.data_index = np.array(list(self.data.keys()))\n    \n    def build_from_train(self, train, n_users, beginner_rate = 0.3, save_name = 'fake_train_generator'):\n        \"\"\"\n        train will be the training set you loaded\n        n_users is a number of user from whom you will sample the data\n        beginner_rate is the rate of these users who will begin their journey during test\n        save_name : the name under which the item will be saved\n        \"\"\"\n        \n        ## Sampling a restricted list of users\n        user_list = train['user_id'].unique()\n        test_user_list = np.random.choice(user_list, size = n_users)\n        train.index = train['user_id']\n        test_data_non_filter = train.loc[test_user_list]\n        test_data_non_filter.index = list(range(test_data_non_filter.shape[0]))\n        \n        ## building a dictionnary with all the rows and container id from a user\n        dico_user = {}\n        def agg(x):\n            return [elt for elt in x]\n        \n        print(\"Generating user dictionnary\")\n        for user, frame in tqdm(test_data_non_filter.groupby('user_id'), total =test_data_non_filter['user_id'].nunique()):\n            if frame.shape[0] > 0:\n                dico_user[user] = {}\n\n                dico_user[user]['min_indice'] = frame['task_container_id'].min()\n                dico_user[user]['max_indice'] = frame['task_container_id'].max()\n\n                r = random.uniform(0,1)\n                if r < beginner_rate:\n                    dico_user[user]['current_indice'] = dico_user[user]['min_indice']\n                else:\n                    dico_user[user]['current_indice'] = random.randint(dico_user[user]['min_indice'],dico_user[user]['max_indice']-2)\n\n                row_ids = frame[['task_container_id','row_id']].groupby('task_container_id').agg(agg)\n                row_ids = row_ids.to_dict()['row_id']\n                dico_user[user]['row_ids'] = row_ids\n\n        work_dico = deepcopy(dico_user)\n        \n        ## Choosing batch_data to generate\n        work_dico = deepcopy(dico_user)\n        batches = {}\n\n        all_rows = []\n        batch_number = 0\n        \n        print('Creating batches')\n        while len(work_dico)> 1:\n\n            size = random.randint(20,500)\n            size = min(size, len(work_dico))\n\n\n            batch = []\n\n            users = np.random.choice(np.array(list(work_dico.keys())),replace = False,  size = size)\n\n            for u in users:\n                try:\n                    batch.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n                    all_rows.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n                    work_dico[u]['current_indice'] += 1\n                    if work_dico[u]['current_indice'] == work_dico[u]['max_indice']:\n                        work_dico.pop(u)\n                except:\n                    work_dico.pop(u)\n\n            batches[batch_number] = batch\n            batch_number += 1\n        \n        ## building data\n\n        data = {}\n        \n        print(\"Building dataset\")\n        test_data_non_filter.index = test_data_non_filter['row_id']\n        for i in tqdm(batches):\n            current_data = test_data_non_filter.loc[np.array(batches[i])]\n            current_data['group_num'] = i\n\n            current_data['prior_group_answers_correct'] = [np.nan for elt in range(current_data.shape[0])]\n            current_data['prior_group_responses'] = [np.nan for elt in range(current_data.shape[0])]\n\n            if i != 0:\n                current_data['prior_group_answers_correct'].iloc[0] = saved_correct_answer\n                current_data['prior_group_responses'].iloc[0] = saved_answer\n\n            saved_answer = str(list(current_data[current_data['content_type_id'] == 0]['user_answer'].values))\n            saved_correct_answer = str(list(current_data[current_data['content_type_id'] == 0]['answered_correctly'].values))\n            current_data = current_data.drop(columns = ['user_answer', 'answered_correctly'])\n\n            data[i] = current_data\n\n        save((data,np.array(all_rows)) , save_name)\n        \n        self.data = data\n        self.all_rows = np.array(all_rows)\n        self.data_index = np.array(list(data.keys()))\n        print('finished')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = FakeDataGenerator()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.load(\"/kaggle/input/fake-api-generation-riid-preprocessed/fake_train_generator\")","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def time(func):\n    def wrapped(*args, **kwargs):\n        t0 = datetime.now()\n        \n        ret = func(*args, **kwargs)\n        \n        dt = datetime.now() - t0\n        print(f\"==> {func.__name__} took {dt.total_seconds():.2} sec\")    \n        return ret\n    return wrapped\n\ndef profile(func):\n    def wrapped(*args, **kwargs):\n        pr = cProfile.Profile()\n        pr.enable()\n        \n        ret = func(*args, **kwargs)\n        \n        pr.disable()\n        s = io.StringIO()\n        sortby = SortKey.CUMULATIVE\n        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n        ps.print_stats(0.02) # print only 2% of the data.\n        print(s.getvalue())\n        return ret\n    return wrapped\n\n@time\ndef preprocess(test_dfm, prev_test_df):\n    \n    # we have prev df's answers, so store the prev_test_df in state_dict\n    if prev_test_df is not None:\n        try:\n            oh_tags = one_hot_tags(prev_test_df[\"tags\"])\n            prev_test_df['answered_correctly'] = list(filter(lambda x: x !=-1, eval(test_dfm['prior_group_answers_correct'].iloc[0])))\n            prev_test_df['answered_correctly'] += 3\n    \n            for i, row in enumerate(prev_test_df.values):\n                user = row[0]\n                # this state\n                s = np.zeros(197)\n                s[:7] = row[1:8] # [1:8] - skip user_id and tags.\n                s[7:] = oh_tags[i]\n                if state_dict.get(user, None) is None: # new user, add to the state dict.\n                    state_dict[user] = s\n                else: # existing user\n                    state = np.vstack([state_dict[user], s]) # append to prev features.\n                    state_dict[user] = state\n        except Exception as e:\n            # pass # do not update the state dict.\n            raise\n        \n    # dummy \"answered_correctly\" - fill token\n    test_dfm[\"answered_correctly\"] = 2\n    \n    # join question for feaures\n    test_df = test_dfm.join(questions_df, on=\"content_id\")\n    \n    # -- process the current df.\n    # selecting required cols\n    test_df = test_df[[\"user_id\", \"timestamp\", \"content_id\", \"task_container_id\", \"prior_question_elapsed_time\", \"prior_question_had_explanation\", \"part\", \"answered_correctly\", \"tags\"]]\n    \n    # 0, 1, 2 are special tokens, so increment 3\n    indicator_cols = [\"content_id\", \"task_container_id\", \"part\", \"prior_question_had_explanation\"]\n    for c in indicator_cols:\n      test_df[c] = test_df[c] + 3\n        \n    # same treatment for the tags\n    test_df[\"tags\"] = test_df[\"tags\"].apply(lambda row: \" \".join([str(int(x)+3) for x in row.split(\" \")]))\n    \n    # FIXME: unseen ids - content_id > 13525, 'task_container_id'> 10002, 'part'> 10\n    test_df.loc[test_df[\"content_id\"] > 13525, \"content_id\"] = 0\n    test_df.loc[test_df[\"task_container_id\"] > 10002, \"task_container_id\"] = 0\n    test_df.loc[test_df[\"part\"] > 10, \"part\"] = 0\n    # test_df.fillna(2, inplace=True) # FILL TOKEN\n    \n    return test_df\n\n@time\ndef one_hot_tags(tags):\n    \n    tags = tf.strings.to_number(tf.strings.split(tags), out_type=tf.int32) # will produce a ragged tensor with tags for each q\n    # ragged tensor of tags [[2], [3, 4]] is converted to one hot like [[0,0,1..], [[0,0,0,1,..], [0,0,0,0,1...]]]\n    # then sumed along axis 1, so for each question there will be 1 for all the tags associated with it.\n    tags = tf.reduce_sum(tf.one_hot(tags, depth=190), axis=1) # shape [seq_len, 190]\n    tags = tags.numpy().astype(np.uint8)\n    \n    return tags\n\n@time\ndef get_x(test_df):\n    max_seq_len = 2\n    xb = []\n    oh_tags = one_hot_tags(test_df[\"tags\"])\n    start_tokens = np.zeros(197)\n    start_tokens[:7] = np.ones(7)\n    for idx, row in enumerate(test_df.values):\n        user = row[0]\n        x = np.zeros((SEQ_LEN+1, 197))\n        if state_dict.get(user, None) is None: # new user\n            if train_state_dict.get(user, None) is None:\n                x[-2, :] = start_tokens\n                # current state\n                x[-1, :7] = row[1:8] # [1:8] - skip user_id and tags.\n                x[-1, 7:] = oh_tags[idx]\n            else:\n                # start tokens\n                x[0, :] = start_tokens\n                \n                # prev states from train set\n                i = train_state_dict[user]\n                x[1:-1, 0] = int64_feat[i] # timestamp\n                x[1:-1, 1:3] = int16_feat[i] # content_id, task_container_id\n                x[1:-1, 3] = int32_feat[i] # prior_question_elapsed_time\n                x[1:-1, 4:7] = int8_feat[i] # prior_q_had_exp, part, answered_correctly\n                x[1:-1, 7:] = tags_feat[i].todense()\n                \n                # current state\n                x[-1, :7] = row[1:8] # [1:] - skip user_id and tags.\n                x[-1, 7:] = oh_tags[idx]\n        else: # existing user, get prev states and build time series.\n            # prev state from test set\n            prev_state = state_dict[user]\n            seq_len = prev_state.shape[0] + 1\n            x[-seq_len:-1, :] = prev_state\n            max_seq_len = max(seq_len, max_seq_len)\n\n            # curr_state\n            x[-1, :7] = row[1:8] # [1:] - skip user_id and tags.\n            x[-1, 7:] = oh_tags[idx]\n            \n            # prev states from train set\n            if train_state_dict.get(user, None):\n                len_to_fill = SEQ_LEN+1 - seq_len\n                i = train_state_dict[user]\n                x[:len_to_fill, 0] = int64_feat[i][-len_to_fill:] # timestamp\n                x[:len_to_fill, 1:3] = int16_feat[i][-len_to_fill:] # content_id, task_container_id\n                x[:len_to_fill, 3] = int32_feat[i][-len_to_fill:] # prior_question_elapsed_time\n                x[:len_to_fill, 4:7] = int8_feat[i][-len_to_fill:] # prior_q_had_exp, part, answered_correctly\n                x[:len_to_fill, 7:] = tags_feat[i][-len_to_fill:].todense()\n                max_seq_len = 513\n            \n        xb.append(x)\n    x = np.stack(xb, axis=0)[:, -max_seq_len:, :]\n    return x\n\n@time\ndef predict(x):\n    \n    # predict\n    preds = model(x, training=False)[:, -1, :].numpy().flatten()\n    # preds = model.predict(x)[:, -1, :].flatten()\n    \n    return preds","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_dict = {}\ntrain_state_dict = dict()\nwith open(\"/kaggle/input/riid-state-dict-512-0-7/user_index.pkl\", \"rb\") as f:\n    train_state_dict = pickle.load(f)\n    \nint64_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int64_feat.npz\").todense()\nint32_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int32_feat.npz\").todense()\nint16_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int16_feat.npz\").todense()\nint8_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/int8_feat.npz\").todense()\ntags_feat = sparse.load_npz(\"/kaggle/input/riid-state-dict-512-0-7/tags_feat.npz\")","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = datetime.now()\ntest_dfs = [] # for debug\n\nprev_test_df = None\n\nfor test_dfm, sample_pred in itertools.islice(iter_test, 10):\n    print(\"\\n*\\n*\\n--- iteration ---\")\n    print(len(test_dfm))\n    t0 = datetime.now()\n    \n    # filtering only questions, removing lectures.\n    test_dfm = test_dfm[test_dfm.content_type_id == False].copy()\n    \n    test_df = preprocess(test_dfm, prev_test_df)\n\n    test_dfs.append(test_df.copy())\n    \n    prev_test_df = test_df.copy()\n    \n    x = get_x(test_df)\n    \n    # final_preds = predict(x)\n        \n    # submit\n    # test_dfm[\"answered_correctly\"] = final_preds\n    \n    # env.predict(test_dfm[[\"row_id\", \"answered_correctly\"]])\n    print(f\"iteration took: {(datetime.now() - t0).total_seconds():.3}s\")\n    \ntotal_time = datetime.now() - start\nprint(f\"===== Total time: {total_time.total_seconds():.3} sec\")","execution_count":24,"outputs":[{"output_type":"stream","text":"\n*\n*\n--- iteration ---\n561\n==> preprocess took 0.014 sec\n==> one_hot_tags took 0.006 sec\n==> get_x took 0.95 sec\niteration took: 0.969s\n\n*\n*\n--- iteration ---\n582\n==> one_hot_tags took 0.006 sec\n==> preprocess took 0.032 sec\n==> one_hot_tags took 0.0056 sec\n==> get_x took 0.79 sec\niteration took: 0.827s\n\n*\n*\n--- iteration ---\n54\n==> one_hot_tags took 0.0072 sec\n==> preprocess took 0.026 sec\n==> one_hot_tags took 0.005 sec\n==> get_x took 0.067 sec\niteration took: 0.0965s\n\n*\n*\n--- iteration ---\n175\n==> one_hot_tags took 0.0063 sec\n==> preprocess took 0.019 sec\n==> one_hot_tags took 0.0049 sec\n==> get_x took 0.24 sec\niteration took: 0.263s\n\n*\n*\n--- iteration ---\n238\n==> one_hot_tags took 0.0058 sec\n==> preprocess took 0.02 sec\n==> one_hot_tags took 0.0048 sec\n==> get_x took 0.33 sec\niteration took: 0.349s\n\n*\n*\n--- iteration ---\n345\n==> one_hot_tags took 0.0055 sec\n==> preprocess took 0.021 sec\n==> one_hot_tags took 0.0053 sec\n==> get_x took 0.47 sec\niteration took: 0.498s\n\n*\n*\n--- iteration ---\n283\n==> one_hot_tags took 0.0057 sec\n==> preprocess took 0.022 sec\n==> one_hot_tags took 0.0051 sec\n==> get_x took 0.38 sec\niteration took: 0.406s\n\n*\n*\n--- iteration ---\n168\n==> one_hot_tags took 0.0061 sec\n==> preprocess took 0.022 sec\n==> one_hot_tags took 0.0056 sec\n==> get_x took 0.23 sec\niteration took: 0.251s\n\n*\n*\n--- iteration ---\n435\n==> one_hot_tags took 0.0052 sec\n==> preprocess took 0.019 sec\n==> one_hot_tags took 0.0049 sec\n==> get_x took 0.59 sec\niteration took: 0.611s\n\n*\n*\n--- iteration ---\n158\n==> one_hot_tags took 0.0062 sec\n==> preprocess took 0.024 sec\n==> one_hot_tags took 0.0046 sec\n==> get_x took 0.21 sec\niteration took: 0.237s\n===== Total time: 4.54 sec\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.iloc[0]","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"user_id                           1500802804\ntimestamp                             215339\ncontent_id                              2949\ntask_container_id                          9\nprior_question_elapsed_time            15000\nprior_question_had_explanation             3\npart                                       7\nanswered_correctly                         2\ntags                               139 70 95\nName: 70526899, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[0, -1, :7]","execution_count":62,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"array([2.15339e+05, 2.94900e+03, 9.00000e+00, 1.50000e+04, 3.00000e+00,\n       7.00000e+00, 2.00000e+00])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(x[0, -1, 7:] == 1) # one hot tags","execution_count":63,"outputs":[{"output_type":"execute_result","execution_count":63,"data":{"text/plain":"(array([ 70,  95, 139]),)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Without train_df user states"},{"metadata":{},"cell_type":"markdown","source":"tf.stack - 500 - 512-sec"},{"metadata":{},"cell_type":"markdown","source":"np.stack - 500 - 443 sec"},{"metadata":{},"cell_type":"markdown","source":"np.stack - 500 - GPU - 82 sec"},{"metadata":{},"cell_type":"markdown","source":"tf.stack - 500 - GPU 142 sec"},{"metadata":{},"cell_type":"markdown","source":"### With train_df user states."},{"metadata":{},"cell_type":"markdown","source":"np.stack 500 GPU - 248 sec"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}